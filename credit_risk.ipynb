{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Modèle de risque de crédit\n\nLe but de ce projet est de prédire à l'aide des informations disponibles quels clients sont les plus susceptibles de ne pas rembourser leurs prêts.","metadata":{}},{"cell_type":"markdown","source":"## Frameworks necessaires","metadata":{}},{"cell_type":"code","source":"!pip install polars","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-31T09:11:34.605036Z","iopub.execute_input":"2024-05-31T09:11:34.605481Z","iopub.status.idle":"2024-05-31T09:11:49.352878Z","shell.execute_reply.started":"2024-05-31T09:11:34.605443Z","shell.execute_reply":"2024-05-31T09:11:49.351458Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: polars in /opt/conda/lib/python3.10/site-packages (0.20.21)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install lightgbm","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-31T09:11:49.354576Z","iopub.execute_input":"2024-05-31T09:11:49.354959Z","iopub.status.idle":"2024-05-31T09:12:04.077919Z","shell.execute_reply.started":"2024-05-31T09:11:49.354923Z","shell.execute_reply":"2024-05-31T09:12:04.076458Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: lightgbm in /opt/conda/lib/python3.10/site-packages (4.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from lightgbm) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from lightgbm) (1.11.4)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --upgrade pandas","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-31T09:12:04.081695Z","iopub.execute_input":"2024-05-31T09:12:04.082206Z","iopub.status.idle":"2024-05-31T09:12:19.314870Z","shell.execute_reply.started":"2024-05-31T09:12:04.082157Z","shell.execute_reply":"2024-05-31T09:12:19.313530Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --upgrade dask lightgbm","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:12:19.316578Z","iopub.execute_input":"2024-05-31T09:12:19.316931Z","iopub.status.idle":"2024-05-31T09:12:38.094154Z","shell.execute_reply.started":"2024-05-31T09:12:19.316897Z","shell.execute_reply":"2024-05-31T09:12:38.092659Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Requirement already satisfied: dask in /opt/conda/lib/python3.10/site-packages (2024.4.1)\nCollecting dask\n  Downloading dask-2024.5.1-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: lightgbm in /opt/conda/lib/python3.10/site-packages (4.2.0)\nCollecting lightgbm\n  Downloading lightgbm-4.3.0-py3-none-manylinux_2_28_x86_64.whl.metadata (19 kB)\nRequirement already satisfied: click>=8.1 in /opt/conda/lib/python3.10/site-packages (from dask) (8.1.7)\nRequirement already satisfied: cloudpickle>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from dask) (2.2.1)\nRequirement already satisfied: fsspec>=2021.09.0 in /opt/conda/lib/python3.10/site-packages (from dask) (2024.2.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from dask) (21.3)\nRequirement already satisfied: partd>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from dask) (1.4.1)\nRequirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from dask) (6.0.1)\nRequirement already satisfied: toolz>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from dask) (0.12.1)\nRequirement already satisfied: importlib-metadata>=4.13.0 in /opt/conda/lib/python3.10/site-packages (from dask) (6.11.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from lightgbm) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from lightgbm) (1.11.4)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=4.13.0->dask) (3.17.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->dask) (3.1.1)\nRequirement already satisfied: locket in /opt/conda/lib/python3.10/site-packages (from partd>=1.2.0->dask) (1.0.0)\nDownloading dask-2024.5.1-py3-none-any.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading lightgbm-4.3.0-py3-none-manylinux_2_28_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: lightgbm, dask\n  Attempting uninstall: lightgbm\n    Found existing installation: lightgbm 4.2.0\n    Uninstalling lightgbm-4.2.0:\n      Successfully uninstalled lightgbm-4.2.0\n  Attempting uninstall: dask\n    Found existing installation: dask 2024.4.1\n    Uninstalling dask-2024.4.1:\n      Successfully uninstalled dask-2024.4.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndask-expr 1.0.11 requires dask==2024.4.1, but you have dask 2024.5.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed dask-2024.5.1 lightgbm-4.3.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Chargement des données","metadata":{}},{"cell_type":"code","source":"import dask\nimport os\nimport polars as pl\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score \n\n#dataPath = \"C:/Users/astri/OneDrive/Documents/M2 DS/PROJETS PERSO/KAGGLE/\"\ndataPath = \"/kaggle/input/home-credit-credit-risk-model-stability/\"","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:25:12.342568Z","iopub.execute_input":"2024-05-31T09:25:12.342998Z","iopub.status.idle":"2024-05-31T09:25:12.349709Z","shell.execute_reply.started":"2024-05-31T09:25:12.342968Z","shell.execute_reply":"2024-05-31T09:25:12.348328Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Set the correct data path\ndata_path = \"C:/Users/astri/OneDrive/Documents/M2 DS/PROJETS PERSO/KAGGLE/csv_files/train/\"\n\n# Check if the file exists\nfile_path = os.path.join(data_path, \"train_base.csv\")\nif os.path.exists(file_path):\n    # Load the CSV file\n    train_basetable = pl.read_csv(file_path)\n    print(\"File loaded successfully!\")\nelse:\n    print(\"File not found. Please check the file path.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:25:17.621880Z","iopub.execute_input":"2024-05-31T09:25:17.622269Z","iopub.status.idle":"2024-05-31T09:25:17.630681Z","shell.execute_reply.started":"2024-05-31T09:25:17.622239Z","shell.execute_reply":"2024-05-31T09:25:17.629201Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Data path: C:/Users/astri/OneDrive/Documents/M2 DS/PROJETS PERSO/KAGGLE/csv_files/train/\nFile not found. Please check the file path.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Ces bouts de code suivants permettent de charger facilement les données.","metadata":{}},{"cell_type":"code","source":"def set_table_dtypes(df: pl.DataFrame) -> pl.DataFrame:\n    # implement here all desired dtypes for tables\n    # the following is just an example\n    for col in df.columns:\n        # last letter of column name will help you determine the type\n        if col[-1] in (\"P\", \"A\"):\n            df = df.with_columns(pl.col(col).cast(pl.Float64).alias(col))\n\n    return df\n\ndef convert_strings(df: pd.DataFrame) -> pd.DataFrame:\n    for col in df.columns:  \n        if df[col].dtype.name in ['object', 'string']:\n            df[col] = df[col].astype(\"string\").astype('category')\n            current_categories = df[col].cat.categories\n            new_categories = current_categories.to_list() + [\"Unknown\"]\n            new_dtype = pd.CategoricalDtype(categories=new_categories, ordered=True)\n            df[col] = df[col].astype(new_dtype)\n    return df\n\n\ntrain_basetable = pl.read_csv(dataPath + \"csv_files/train/train_base.csv\")\ntrain_static = pl.concat(\n    [\n        pl.read_csv(dataPath + \"csv_files/train/train_static_0_0.csv\").pipe(set_table_dtypes),\n        pl.read_csv(dataPath + \"csv_files/train/train_static_0_1.csv\").pipe(set_table_dtypes),\n    ],\n    how=\"vertical_relaxed\",\n)\ntrain_static_cb = pl.read_csv(dataPath + \"csv_files/train/train_static_cb_0.csv\").pipe(set_table_dtypes)\ntrain_person_1 = pl.read_csv(dataPath + \"csv_files/train/train_person_1.csv\").pipe(set_table_dtypes) \ntrain_credit_bureau_b_2 = pl.read_csv(dataPath + \"csv_files/train/train_credit_bureau_b_2.csv\").pipe(set_table_dtypes) \n","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:25:49.934136Z","iopub.execute_input":"2024-05-31T09:25:49.934532Z","iopub.status.idle":"2024-05-31T09:26:05.286112Z","shell.execute_reply.started":"2024-05-31T09:25:49.934501Z","shell.execute_reply":"2024-05-31T09:26:05.284897Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_basetable = pl.read_csv(dataPath + \"csv_files/test/test_base.csv\")\ntest_static = pl.concat(\n    [\n        pl.read_csv(dataPath + \"csv_files/test/test_static_0_0.csv\").pipe(set_table_dtypes),\n        pl.read_csv(dataPath + \"csv_files/test/test_static_0_1.csv\").pipe(set_table_dtypes),\n        pl.read_csv(dataPath + \"csv_files/test/test_static_0_2.csv\").pipe(set_table_dtypes),\n    ],\n    how=\"vertical_relaxed\",\n)\ntest_static_cb = pl.read_csv(dataPath + \"csv_files/test/test_static_cb_0.csv\").pipe(set_table_dtypes)\ntest_person_1 = pl.read_csv(dataPath + \"csv_files/test/test_person_1.csv\").pipe(set_table_dtypes) \ntest_credit_bureau_b_2 = pl.read_csv(dataPath + \"csv_files/test/test_credit_bureau_b_2.csv\").pipe(set_table_dtypes) ","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:26:22.193255Z","iopub.execute_input":"2024-05-31T09:26:22.194762Z","iopub.status.idle":"2024-05-31T09:26:22.241495Z","shell.execute_reply.started":"2024-05-31T09:26:22.194708Z","shell.execute_reply":"2024-05-31T09:26:22.240232Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"##Traitement de données","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:26:22.243704Z","iopub.execute_input":"2024-05-31T09:26:22.244142Z","iopub.status.idle":"2024-05-31T09:26:22.249385Z","shell.execute_reply.started":"2024-05-31T09:26:22.244111Z","shell.execute_reply":"2024-05-31T09:26:22.247847Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# We need to use aggregation functions in tables with depth > 1, so tables that contain num_group1 column or \n# also num_group2 column.\ntrain_person_1_feats_1 = train_person_1.group_by(\"case_id\").agg(\n    pl.col(\"mainoccupationinc_384A\").max().alias(\"mainoccupationinc_384A_max\"),\n    (pl.col(\"incometype_1044T\") == \"SELFEMPLOYED\").max().alias(\"mainoccupationinc_384A_any_selfemployed\")\n)\n\n# Here num_group1=0 has special meaning, it is the person who applied for the loan.\ntrain_person_1_feats_2 = train_person_1.select([\"case_id\", \"num_group1\", \"housetype_905L\"]).filter(\n    pl.col(\"num_group1\") == 0\n).drop(\"num_group1\").rename({\"housetype_905L\": \"person_housetype\"})\n\n# Here we have num_goup1 and num_group2, so we need to aggregate again.\ntrain_credit_bureau_b_2_feats = train_credit_bureau_b_2.group_by(\"case_id\").agg(\n    pl.col(\"pmts_pmtsoverdue_635A\").max().alias(\"pmts_pmtsoverdue_635A_max\"),\n    (pl.col(\"pmts_dpdvalue_108P\") > 31).max().alias(\"pmts_dpdvalue_108P_over31\")\n)\n\n# We will process in this examples only A-type and M-type columns, so we need to select them.\nselected_static_cols = []\nfor col in train_static.columns:\n    if col[-1] in (\"A\", \"M\"):\n        selected_static_cols.append(col)\nprint(selected_static_cols)\n\nselected_static_cb_cols = []\nfor col in train_static_cb.columns:\n    if col[-1] in (\"A\", \"M\"):\n        selected_static_cb_cols.append(col)\nprint(selected_static_cb_cols)\n\n# Join all tables together.\ndata = train_basetable.join(\n    train_static.select([\"case_id\"]+selected_static_cols), how=\"left\", on=\"case_id\"\n).join(\n    train_static_cb.select([\"case_id\"]+selected_static_cb_cols), how=\"left\", on=\"case_id\"\n).join(\n    train_person_1_feats_1, how=\"left\", on=\"case_id\"\n).join(\n    train_person_1_feats_2, how=\"left\", on=\"case_id\"\n).join(\n    train_credit_bureau_b_2_feats, how=\"left\", on=\"case_id\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:26:22.252113Z","iopub.execute_input":"2024-05-31T09:26:22.252594Z","iopub.status.idle":"2024-05-31T09:26:25.860749Z","shell.execute_reply.started":"2024-05-31T09:26:22.252551Z","shell.execute_reply":"2024-05-31T09:26:25.859686Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"['amtinstpaidbefduel24m_4187115A', 'annuity_780A', 'annuitynextmonth_57A', 'avginstallast24m_3658937A', 'avglnamtstart24m_4525187A', 'avgoutstandbalancel6m_4187114A', 'avgpmtlast12m_4525200A', 'credamount_770A', 'currdebt_22A', 'currdebtcredtyperange_828A', 'disbursedcredamount_1113A', 'downpmt_116A', 'inittransactionamount_650A', 'lastapprcommoditycat_1041M', 'lastapprcommoditytypec_5251766M', 'lastapprcredamount_781A', 'lastcancelreason_561M', 'lastotherinc_902A', 'lastotherlnsexpense_631A', 'lastrejectcommoditycat_161M', 'lastrejectcommodtypec_5251769M', 'lastrejectcredamount_222A', 'lastrejectreason_759M', 'lastrejectreasonclient_4145040M', 'maininc_215A', 'maxannuity_159A', 'maxannuity_4075009A', 'maxdebt4_972A', 'maxinstallast24m_3658928A', 'maxlnamtstart6m_4525199A', 'maxoutstandbalancel12m_4187113A', 'maxpmtlast3m_4525190A', 'previouscontdistrict_112M', 'price_1097A', 'sumoutstandtotal_3546847A', 'sumoutstandtotalest_4493215A', 'totaldebt_9A', 'totalsettled_863A', 'totinstallast1m_4525188A']\n['description_5085714M', 'education_1103M', 'education_88M', 'maritalst_385M', 'maritalst_893M', 'pmtaverage_3A', 'pmtaverage_4527227A', 'pmtaverage_4955615A', 'pmtssum_45A']\n","output_type":"stream"}]},{"cell_type":"code","source":"test_person_1_feats_1 = test_person_1.group_by(\"case_id\").agg(\n    pl.col(\"mainoccupationinc_384A\").max().alias(\"mainoccupationinc_384A_max\"),\n    (pl.col(\"incometype_1044T\") == \"SELFEMPLOYED\").max().alias(\"mainoccupationinc_384A_any_selfemployed\")\n)\n\ntest_person_1_feats_2 = test_person_1.select([\"case_id\", \"num_group1\", \"housetype_905L\"]).filter(\n    pl.col(\"num_group1\") == 0\n).drop(\"num_group1\").rename({\"housetype_905L\": \"person_housetype\"})\n\ntest_credit_bureau_b_2_feats = test_credit_bureau_b_2.group_by(\"case_id\").agg(\n    pl.col(\"pmts_pmtsoverdue_635A\").max().alias(\"pmts_pmtsoverdue_635A_max\"),\n    (pl.col(\"pmts_dpdvalue_108P\") > 31).max().alias(\"pmts_dpdvalue_108P_over31\")\n)\n\ndata_submission = test_basetable.join(\n    test_static.select([\"case_id\"]+selected_static_cols), how=\"left\", on=\"case_id\"\n).join(\n    test_static_cb.select([\"case_id\"]+selected_static_cb_cols), how=\"left\", on=\"case_id\"\n).join(\n    test_person_1_feats_1, how=\"left\", on=\"case_id\"\n).join(\n    test_person_1_feats_2, how=\"left\", on=\"case_id\"\n).join(\n    test_credit_bureau_b_2_feats, how=\"left\", on=\"case_id\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:26:25.861869Z","iopub.execute_input":"2024-05-31T09:26:25.862192Z","iopub.status.idle":"2024-05-31T09:26:25.877987Z","shell.execute_reply.started":"2024-05-31T09:26:25.862165Z","shell.execute_reply":"2024-05-31T09:26:25.876978Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"case_ids = data[\"case_id\"].unique().shuffle(seed=1)\ncase_ids_train, case_ids_test = train_test_split(case_ids, train_size=0.6, random_state=1)\ncase_ids_valid, case_ids_test = train_test_split(case_ids_test, train_size=0.5, random_state=1)\n\ncols_pred = []\nfor col in data.columns:\n    if col[-1].isupper() and col[:-1].islower():\n        cols_pred.append(col)\n\nprint(cols_pred)\n\ndef from_polars_to_pandas(case_ids: pl.DataFrame) -> pl.DataFrame:\n    return (\n        data.filter(pl.col(\"case_id\").is_in(case_ids))[[\"case_id\", \"WEEK_NUM\", \"target\"]].to_pandas(),\n        data.filter(pl.col(\"case_id\").is_in(case_ids))[cols_pred].to_pandas(),\n        data.filter(pl.col(\"case_id\").is_in(case_ids))[\"target\"].to_pandas()\n    )\n\nbase_train, X_train, y_train = from_polars_to_pandas(case_ids_train)\nbase_valid, X_valid, y_valid = from_polars_to_pandas(case_ids_valid)\nbase_test, X_test, y_test = from_polars_to_pandas(case_ids_test)\n\nfor df in [X_train, X_valid, X_test]:\n    df = convert_strings(df)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:26:25.880736Z","iopub.execute_input":"2024-05-31T09:26:25.881143Z","iopub.status.idle":"2024-05-31T09:26:34.703046Z","shell.execute_reply.started":"2024-05-31T09:26:25.881114Z","shell.execute_reply":"2024-05-31T09:26:34.701572Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"['amtinstpaidbefduel24m_4187115A', 'annuity_780A', 'annuitynextmonth_57A', 'avginstallast24m_3658937A', 'avglnamtstart24m_4525187A', 'avgoutstandbalancel6m_4187114A', 'avgpmtlast12m_4525200A', 'credamount_770A', 'currdebt_22A', 'currdebtcredtyperange_828A', 'disbursedcredamount_1113A', 'downpmt_116A', 'inittransactionamount_650A', 'lastapprcommoditycat_1041M', 'lastapprcommoditytypec_5251766M', 'lastapprcredamount_781A', 'lastcancelreason_561M', 'lastotherinc_902A', 'lastotherlnsexpense_631A', 'lastrejectcommoditycat_161M', 'lastrejectcommodtypec_5251769M', 'lastrejectcredamount_222A', 'lastrejectreason_759M', 'lastrejectreasonclient_4145040M', 'maininc_215A', 'maxannuity_159A', 'maxannuity_4075009A', 'maxdebt4_972A', 'maxinstallast24m_3658928A', 'maxlnamtstart6m_4525199A', 'maxoutstandbalancel12m_4187113A', 'maxpmtlast3m_4525190A', 'previouscontdistrict_112M', 'price_1097A', 'sumoutstandtotal_3546847A', 'sumoutstandtotalest_4493215A', 'totaldebt_9A', 'totalsettled_863A', 'totinstallast1m_4525188A', 'description_5085714M', 'education_1103M', 'education_88M', 'maritalst_385M', 'maritalst_893M', 'pmtaverage_3A', 'pmtaverage_4527227A', 'pmtaverage_4955615A', 'pmtssum_45A']\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"Train: {X_train.shape}\")\nprint(f\"Valid: {X_valid.shape}\")\nprint(f\"Test: {X_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:54:17.869752Z","iopub.execute_input":"2024-05-31T09:54:17.870184Z","iopub.status.idle":"2024-05-31T09:54:17.876268Z","shell.execute_reply.started":"2024-05-31T09:54:17.870149Z","shell.execute_reply":"2024-05-31T09:54:17.874994Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Train: (915995, 48)\nValid: (305332, 48)\nTest: (305332, 48)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Ces deux ensembles de données X_train et base_train seront utilisés ensemble pour l'entraînement d'un modèle des modèles. X_train contient les caractéristiques sur lesquelles le modèle sera entraîné, tandis que base_train contient les étiquettes cibles (ou les valeurs à prédire) ainsi que les scores associés","metadata":{}},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2024-05-31T14:20:09.376346Z","iopub.execute_input":"2024-05-31T14:20:09.376981Z","iopub.status.idle":"2024-05-31T14:20:09.472727Z","shell.execute_reply.started":"2024-05-31T14:20:09.376941Z","shell.execute_reply":"2024-05-31T14:20:09.470798Z"},"trusted":true},"execution_count":123,"outputs":[{"execution_count":123,"output_type":"execute_result","data":{"text/plain":"        amtinstpaidbefduel24m_4187115A  annuity_780A  annuitynextmonth_57A  \\\n0                                  NaN   3390.199951              0.000000   \n1                                  NaN   9568.600586              0.000000   \n2                                  NaN   5109.600098              0.000000   \n3                                  NaN   2581.000000              0.000000   \n4                                  NaN   2400.000000              0.000000   \n...                                ...           ...                   ...   \n305327                   119089.992188   4138.399902              0.000000   \n305328                        0.000000   4747.200195              0.000000   \n305329                   335469.250000   7088.600098           7216.000000   \n305330                   169487.718750   4960.800293           2717.199951   \n305331                    45531.800781   5192.600098              0.000000   \n\n        avginstallast24m_3658937A  avglnamtstart24m_4525187A  \\\n0                             NaN                        NaN   \n1                             NaN                        NaN   \n2                             NaN                        NaN   \n3                             NaN                        NaN   \n4                             NaN                        NaN   \n...                           ...                        ...   \n305327                5671.000000                        NaN   \n305328                        NaN                        NaN   \n305329               13376.600586                        NaN   \n305330                7369.000000                        NaN   \n305331                4139.200195                        NaN   \n\n        avgoutstandbalancel6m_4187114A  avgpmtlast12m_4525200A  \\\n0                                  NaN                     NaN   \n1                                  NaN                     NaN   \n2                                  NaN                     NaN   \n3                                  NaN                     NaN   \n4                                  NaN                     NaN   \n...                                ...                     ...   \n305327                    20909.320312             6878.800293   \n305328                             NaN                     NaN   \n305329                   109874.585938            14549.000000   \n305330                    12492.797852            10033.200195   \n305331                             NaN             4137.800293   \n\n        credamount_770A  currdebt_22A  currdebtcredtyperange_828A  ...  \\\n0               44000.0      0.000000                       0.000  ...   \n1              100000.0      0.000000                       0.000  ...   \n2               80000.0      0.000000                       0.000  ...   \n3               28000.0      0.000000                       0.000  ...   \n4               40000.0      0.000000                       0.000  ...   \n...                 ...           ...                         ...  ...   \n305327          40000.0      0.000000                       0.000  ...   \n305328          60000.0      0.000000                       0.000  ...   \n305329         100000.0  87968.875000                   87968.875  ...   \n305330          60000.0   7647.200195                       0.000  ...   \n305331          40000.0      0.000000                       0.000  ...   \n\n        totinstallast1m_4525188A  description_5085714M  education_1103M  \\\n0                            NaN                  -1.0             -1.0   \n1                            NaN                  -1.0             -1.0   \n2                            NaN                  -1.0             -1.0   \n3                            NaN                  -1.0             -1.0   \n4                            NaN                  -1.0             -1.0   \n...                          ...                   ...              ...   \n305327              12445.384766                   0.0              2.0   \n305328                       NaN                   0.0              1.0   \n305329               7216.000000                   0.0              2.0   \n305330               2717.199951                   0.0              2.0   \n305331                       NaN                   0.0              1.0   \n\n        education_88M  maritalst_385M  maritalst_893M  pmtaverage_3A  \\\n0                -1.0            -1.0            -1.0            NaN   \n1                -1.0            -1.0            -1.0            NaN   \n2                -1.0            -1.0            -1.0            NaN   \n3                -1.0            -1.0            -1.0            NaN   \n4                -1.0            -1.0            -1.0            NaN   \n...               ...             ...             ...            ...   \n305327            3.0             0.0             3.0            NaN   \n305328            0.0             0.0             1.0            NaN   \n305329            3.0             0.0             3.0            NaN   \n305330            3.0             0.0             3.0            NaN   \n305331            3.0             0.0             3.0            NaN   \n\n        pmtaverage_4527227A  pmtaverage_4955615A  pmtssum_45A  \n0                       NaN                  NaN          NaN  \n1                       NaN                  NaN          NaN  \n2                       NaN                  NaN          NaN  \n3                       NaN                  NaN          NaN  \n4                       NaN                  NaN          NaN  \n...                     ...                  ...          ...  \n305327                  NaN         16097.200195          NaN  \n305328                  NaN                  NaN          NaN  \n305329                  NaN         20508.201172          NaN  \n305330                  NaN                  NaN          NaN  \n305331                  NaN                  NaN          NaN  \n\n[305332 rows x 48 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>amtinstpaidbefduel24m_4187115A</th>\n      <th>annuity_780A</th>\n      <th>annuitynextmonth_57A</th>\n      <th>avginstallast24m_3658937A</th>\n      <th>avglnamtstart24m_4525187A</th>\n      <th>avgoutstandbalancel6m_4187114A</th>\n      <th>avgpmtlast12m_4525200A</th>\n      <th>credamount_770A</th>\n      <th>currdebt_22A</th>\n      <th>currdebtcredtyperange_828A</th>\n      <th>...</th>\n      <th>totinstallast1m_4525188A</th>\n      <th>description_5085714M</th>\n      <th>education_1103M</th>\n      <th>education_88M</th>\n      <th>maritalst_385M</th>\n      <th>maritalst_893M</th>\n      <th>pmtaverage_3A</th>\n      <th>pmtaverage_4527227A</th>\n      <th>pmtaverage_4955615A</th>\n      <th>pmtssum_45A</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>3390.199951</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>44000.0</td>\n      <td>0.000000</td>\n      <td>0.000</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>9568.600586</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>100000.0</td>\n      <td>0.000000</td>\n      <td>0.000</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>5109.600098</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>80000.0</td>\n      <td>0.000000</td>\n      <td>0.000</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>2581.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>28000.0</td>\n      <td>0.000000</td>\n      <td>0.000</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>2400.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>40000.0</td>\n      <td>0.000000</td>\n      <td>0.000</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>305327</th>\n      <td>119089.992188</td>\n      <td>4138.399902</td>\n      <td>0.000000</td>\n      <td>5671.000000</td>\n      <td>NaN</td>\n      <td>20909.320312</td>\n      <td>6878.800293</td>\n      <td>40000.0</td>\n      <td>0.000000</td>\n      <td>0.000</td>\n      <td>...</td>\n      <td>12445.384766</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>16097.200195</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>305328</th>\n      <td>0.000000</td>\n      <td>4747.200195</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>60000.0</td>\n      <td>0.000000</td>\n      <td>0.000</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>305329</th>\n      <td>335469.250000</td>\n      <td>7088.600098</td>\n      <td>7216.000000</td>\n      <td>13376.600586</td>\n      <td>NaN</td>\n      <td>109874.585938</td>\n      <td>14549.000000</td>\n      <td>100000.0</td>\n      <td>87968.875000</td>\n      <td>87968.875</td>\n      <td>...</td>\n      <td>7216.000000</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>20508.201172</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>305330</th>\n      <td>169487.718750</td>\n      <td>4960.800293</td>\n      <td>2717.199951</td>\n      <td>7369.000000</td>\n      <td>NaN</td>\n      <td>12492.797852</td>\n      <td>10033.200195</td>\n      <td>60000.0</td>\n      <td>7647.200195</td>\n      <td>0.000</td>\n      <td>...</td>\n      <td>2717.199951</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>305331</th>\n      <td>45531.800781</td>\n      <td>5192.600098</td>\n      <td>0.000000</td>\n      <td>4139.200195</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4137.800293</td>\n      <td>40000.0</td>\n      <td>0.000000</td>\n      <td>0.000</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>305332 rows × 48 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"X_train.describe()","metadata":{"execution":{"iopub.status.busy":"2024-05-31T15:10:41.283698Z","iopub.execute_input":"2024-05-31T15:10:41.284251Z","iopub.status.idle":"2024-05-31T15:10:44.797178Z","shell.execute_reply.started":"2024-05-31T15:10:41.284216Z","shell.execute_reply":"2024-05-31T15:10:44.795714Z"},"trusted":true},"execution_count":128,"outputs":[{"execution_count":128,"output_type":"execute_result","data":{"text/plain":"       amtinstpaidbefduel24m_4187115A   annuity_780A  annuitynextmonth_57A  \\\ncount                    5.791220e+05  915995.000000         915994.000000   \nmean                     5.605103e+04    4039.998047           1438.738403   \nstd                      7.168848e+04    3011.037109           2809.423584   \nmin                      0.000000e+00      83.000000              0.000000   \n25%                      7.451650e+03    1968.200073              0.000000   \n50%                      2.975565e+04    3151.600098              0.000000   \n75%                      7.652577e+04    5231.399902           2038.800049   \nmax                      1.198913e+06   91601.398438          71878.601562   \n\n       avginstallast24m_3658937A  avglnamtstart24m_4525187A  \\\ncount              541137.000000               97373.000000   \nmean                 5399.340820               44660.792969   \nstd                  6436.574707               44763.152344   \nmin                     0.000000                   0.000000   \n25%                  2529.800049               15685.799805   \n50%                  4071.600098               28460.800781   \n75%                  6553.000000               56377.800781   \nmax                496148.812500              513520.000000   \n\n       avgoutstandbalancel6m_4187114A  avgpmtlast12m_4525200A  \\\ncount                    4.113760e+05           299622.000000   \nmean                     4.598377e+04             6398.227051   \nstd                      6.441692e+04             9204.316406   \nmin                     -7.588198e+06                0.000000   \n25%                      8.719109e+03             2595.199951   \n50%                      2.278338e+04             4422.000000   \n75%                      5.536972e+04             7516.200195   \nmax                      1.131136e+06           495910.406250   \n\n       credamount_770A  currdebt_22A  currdebtcredtyperange_828A  ...  \\\ncount    915995.000000  9.159940e+05                9.159940e+05  ...   \nmean      49875.789062  1.971437e+04                1.101732e+04  ...   \nstd       44182.878906  5.090623e+04                3.683703e+04  ...   \nmin        2000.000000  0.000000e+00                0.000000e+00  ...   \n25%       19998.000000  0.000000e+00                0.000000e+00  ...   \n50%       35190.000000  0.000000e+00                0.000000e+00  ...   \n75%       63984.000000  1.355695e+04                0.000000e+00  ...   \nmax      950000.000000  1.210629e+06                1.028338e+06  ...   \n\n       totinstallast1m_4525188A  description_5085714M  education_1103M  \\\ncount             211305.000000         915995.000000    915995.000000   \nmean               10417.497070              0.845033         2.160383   \nstd                16187.276367              0.406428         1.061152   \nmin                    0.222000             -1.000000        -1.000000   \n25%                 3314.600098              1.000000         1.000000   \n50%                 6216.000000              1.000000         3.000000   \n75%                11697.600586              1.000000         3.000000   \nmax               794899.187500              1.000000         4.000000   \n\n       education_88M  maritalst_385M  maritalst_893M  pmtaverage_3A  \\\ncount  915995.000000   915995.000000   915995.000000   86118.000000   \nmean        2.904335        1.423946        2.902839    9314.902344   \nstd         0.584711        1.260771        0.577290    5568.039062   \nmin        -1.000000       -1.000000       -1.000000       0.000000   \n25%         3.000000        0.000000        3.000000    6590.600098   \n50%         3.000000        2.000000        3.000000    7305.899902   \n75%         3.000000        2.000000        3.000000   13027.474609   \nmax         4.000000        5.000000        5.000000  145257.406250   \n\n       pmtaverage_4527227A  pmtaverage_4955615A    pmtssum_45A  \ncount         68821.000000         43159.000000  343834.000000  \nmean          10052.182617         17616.445312   13232.382812  \nstd            5530.822266          6777.177734   18190.787109  \nmin               5.000000             6.000000       0.000000  \n25%            7192.200195         13649.400391    3167.616211  \n50%            7553.000000         15765.600586    8400.000000  \n75%           13464.400391         21829.500000   17005.658203  \nmax          205848.609375         99085.398438  476843.406250  \n\n[8 rows x 48 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>amtinstpaidbefduel24m_4187115A</th>\n      <th>annuity_780A</th>\n      <th>annuitynextmonth_57A</th>\n      <th>avginstallast24m_3658937A</th>\n      <th>avglnamtstart24m_4525187A</th>\n      <th>avgoutstandbalancel6m_4187114A</th>\n      <th>avgpmtlast12m_4525200A</th>\n      <th>credamount_770A</th>\n      <th>currdebt_22A</th>\n      <th>currdebtcredtyperange_828A</th>\n      <th>...</th>\n      <th>totinstallast1m_4525188A</th>\n      <th>description_5085714M</th>\n      <th>education_1103M</th>\n      <th>education_88M</th>\n      <th>maritalst_385M</th>\n      <th>maritalst_893M</th>\n      <th>pmtaverage_3A</th>\n      <th>pmtaverage_4527227A</th>\n      <th>pmtaverage_4955615A</th>\n      <th>pmtssum_45A</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>5.791220e+05</td>\n      <td>915995.000000</td>\n      <td>915994.000000</td>\n      <td>541137.000000</td>\n      <td>97373.000000</td>\n      <td>4.113760e+05</td>\n      <td>299622.000000</td>\n      <td>915995.000000</td>\n      <td>9.159940e+05</td>\n      <td>9.159940e+05</td>\n      <td>...</td>\n      <td>211305.000000</td>\n      <td>915995.000000</td>\n      <td>915995.000000</td>\n      <td>915995.000000</td>\n      <td>915995.000000</td>\n      <td>915995.000000</td>\n      <td>86118.000000</td>\n      <td>68821.000000</td>\n      <td>43159.000000</td>\n      <td>343834.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>5.605103e+04</td>\n      <td>4039.998047</td>\n      <td>1438.738403</td>\n      <td>5399.340820</td>\n      <td>44660.792969</td>\n      <td>4.598377e+04</td>\n      <td>6398.227051</td>\n      <td>49875.789062</td>\n      <td>1.971437e+04</td>\n      <td>1.101732e+04</td>\n      <td>...</td>\n      <td>10417.497070</td>\n      <td>0.845033</td>\n      <td>2.160383</td>\n      <td>2.904335</td>\n      <td>1.423946</td>\n      <td>2.902839</td>\n      <td>9314.902344</td>\n      <td>10052.182617</td>\n      <td>17616.445312</td>\n      <td>13232.382812</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>7.168848e+04</td>\n      <td>3011.037109</td>\n      <td>2809.423584</td>\n      <td>6436.574707</td>\n      <td>44763.152344</td>\n      <td>6.441692e+04</td>\n      <td>9204.316406</td>\n      <td>44182.878906</td>\n      <td>5.090623e+04</td>\n      <td>3.683703e+04</td>\n      <td>...</td>\n      <td>16187.276367</td>\n      <td>0.406428</td>\n      <td>1.061152</td>\n      <td>0.584711</td>\n      <td>1.260771</td>\n      <td>0.577290</td>\n      <td>5568.039062</td>\n      <td>5530.822266</td>\n      <td>6777.177734</td>\n      <td>18190.787109</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000e+00</td>\n      <td>83.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-7.588198e+06</td>\n      <td>0.000000</td>\n      <td>2000.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>...</td>\n      <td>0.222000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>0.000000</td>\n      <td>5.000000</td>\n      <td>6.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>7.451650e+03</td>\n      <td>1968.200073</td>\n      <td>0.000000</td>\n      <td>2529.800049</td>\n      <td>15685.799805</td>\n      <td>8.719109e+03</td>\n      <td>2595.199951</td>\n      <td>19998.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>...</td>\n      <td>3314.600098</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>6590.600098</td>\n      <td>7192.200195</td>\n      <td>13649.400391</td>\n      <td>3167.616211</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2.975565e+04</td>\n      <td>3151.600098</td>\n      <td>0.000000</td>\n      <td>4071.600098</td>\n      <td>28460.800781</td>\n      <td>2.278338e+04</td>\n      <td>4422.000000</td>\n      <td>35190.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>...</td>\n      <td>6216.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>7305.899902</td>\n      <td>7553.000000</td>\n      <td>15765.600586</td>\n      <td>8400.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>7.652577e+04</td>\n      <td>5231.399902</td>\n      <td>2038.800049</td>\n      <td>6553.000000</td>\n      <td>56377.800781</td>\n      <td>5.536972e+04</td>\n      <td>7516.200195</td>\n      <td>63984.000000</td>\n      <td>1.355695e+04</td>\n      <td>0.000000e+00</td>\n      <td>...</td>\n      <td>11697.600586</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>13027.474609</td>\n      <td>13464.400391</td>\n      <td>21829.500000</td>\n      <td>17005.658203</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.198913e+06</td>\n      <td>91601.398438</td>\n      <td>71878.601562</td>\n      <td>496148.812500</td>\n      <td>513520.000000</td>\n      <td>1.131136e+06</td>\n      <td>495910.406250</td>\n      <td>950000.000000</td>\n      <td>1.210629e+06</td>\n      <td>1.028338e+06</td>\n      <td>...</td>\n      <td>794899.187500</td>\n      <td>1.000000</td>\n      <td>4.000000</td>\n      <td>4.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>145257.406250</td>\n      <td>205848.609375</td>\n      <td>99085.398438</td>\n      <td>476843.406250</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 48 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"base_train","metadata":{"execution":{"iopub.status.busy":"2024-05-31T14:29:07.043476Z","iopub.execute_input":"2024-05-31T14:29:07.043992Z","iopub.status.idle":"2024-05-31T14:29:07.061953Z","shell.execute_reply.started":"2024-05-31T14:29:07.043958Z","shell.execute_reply":"2024-05-31T14:29:07.060349Z"},"trusted":true},"execution_count":126,"outputs":[{"execution_count":126,"output_type":"execute_result","data":{"text/plain":"        case_id  WEEK_NUM  target     score\n0             0         0       0  0.043662\n1             2         0       0  0.048437\n2             5         0       0  0.030461\n3             6         0       0  0.072764\n4             7         0       0  0.026860\n...         ...       ...     ...       ...\n915990  2703449        91       0  0.076109\n915991  2703450        91       0  0.005971\n915992  2703452        91       0  0.024759\n915993  2703453        91       0  0.005225\n915994  2703454        91       0  0.004325\n\n[915995 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>case_id</th>\n      <th>WEEK_NUM</th>\n      <th>target</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.043662</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.048437</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.030461</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.072764</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.026860</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>915990</th>\n      <td>2703449</td>\n      <td>91</td>\n      <td>0</td>\n      <td>0.076109</td>\n    </tr>\n    <tr>\n      <th>915991</th>\n      <td>2703450</td>\n      <td>91</td>\n      <td>0</td>\n      <td>0.005971</td>\n    </tr>\n    <tr>\n      <th>915992</th>\n      <td>2703452</td>\n      <td>91</td>\n      <td>0</td>\n      <td>0.024759</td>\n    </tr>\n    <tr>\n      <th>915993</th>\n      <td>2703453</td>\n      <td>91</td>\n      <td>0</td>\n      <td>0.005225</td>\n    </tr>\n    <tr>\n      <th>915994</th>\n      <td>2703454</td>\n      <td>91</td>\n      <td>0</td>\n      <td>0.004325</td>\n    </tr>\n  </tbody>\n</table>\n<p>915995 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"base_train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-05-31T15:14:52.747239Z","iopub.execute_input":"2024-05-31T15:14:52.749012Z","iopub.status.idle":"2024-05-31T15:14:52.769136Z","shell.execute_reply.started":"2024-05-31T15:14:52.748947Z","shell.execute_reply":"2024-05-31T15:14:52.767484Z"},"trusted":true},"execution_count":134,"outputs":[{"execution_count":134,"output_type":"execute_result","data":{"text/plain":"case_id     0\nWEEK_NUM    0\ntarget      0\nscore       0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"base_train.describe()","metadata":{"execution":{"iopub.status.busy":"2024-05-31T15:15:27.495702Z","iopub.execute_input":"2024-05-31T15:15:27.496207Z","iopub.status.idle":"2024-05-31T15:15:27.650448Z","shell.execute_reply.started":"2024-05-31T15:15:27.496169Z","shell.execute_reply":"2024-05-31T15:15:27.648715Z"},"trusted":true},"execution_count":136,"outputs":[{"execution_count":136,"output_type":"execute_result","data":{"text/plain":"            case_id       WEEK_NUM         target          score\ncount  9.159950e+05  915995.000000  915995.000000  915995.000000\nmean   1.285867e+06      40.766140       0.031520       0.031487\nstd    7.189843e+05      23.798236       0.174718       0.030791\nmin    0.000000e+00       0.000000       0.000000       0.001321\n25%    7.658125e+05      23.000000       0.000000       0.013380\n50%    1.357130e+06      40.000000       0.000000       0.023322\n75%    1.738910e+06      55.000000       0.000000       0.039356\nmax    2.703454e+06      91.000000       1.000000       0.831328","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>case_id</th>\n      <th>WEEK_NUM</th>\n      <th>target</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>9.159950e+05</td>\n      <td>915995.000000</td>\n      <td>915995.000000</td>\n      <td>915995.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.285867e+06</td>\n      <td>40.766140</td>\n      <td>0.031520</td>\n      <td>0.031487</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>7.189843e+05</td>\n      <td>23.798236</td>\n      <td>0.174718</td>\n      <td>0.030791</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.001321</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>7.658125e+05</td>\n      <td>23.000000</td>\n      <td>0.000000</td>\n      <td>0.013380</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.357130e+06</td>\n      <td>40.000000</td>\n      <td>0.000000</td>\n      <td>0.023322</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.738910e+06</td>\n      <td>55.000000</td>\n      <td>0.000000</td>\n      <td>0.039356</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2.703454e+06</td>\n      <td>91.000000</td>\n      <td>1.000000</td>\n      <td>0.831328</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Nous constatons que nos données on beaucoup de valeurs manquantes, mais ce n'est pas grave car les modèles que nous allons utiliser ont la capacité de gérer ces valeurs manquantes lors de leur entrainement.","metadata":{}},{"cell_type":"markdown","source":"Voici l'explication de quelques variables de ces données: \\\n**case_id**: Un identifiant unique pour chaque observation.\\\n**WEEK_NUM**: La semaine à laquelle l'observation fait référence.\\\n**target**: La variable cible,la valeur que le modèle essaie de prédire.\\\n**score**: le score associé à chaque observation.\n\nLes variables de X_train représentesnt les iformations des clients comme le régime matrimonial, le type d'éducation, l'adresse postale, le niveau de transaction bancaire, le montant emprunté, le taux d'intérêt, et bien d'autres paramètres.\n\nAyant donc ces informations, on pourea par exemple faire une analyse en composantes principales ou une sélection de variables pour observer quelles variables influencent le plus sur le score d'un client. Mais nous n'allons pas le faire car il faudra supprimer les données qui on des lignes manquantes et cela pourra un peu biaiser notre modèle final. Mais rein n'empêche de le faire.","metadata":{}},{"cell_type":"markdown","source":"Après avoir chargé nos données train, validation et test, nous pouvons à présent les traiter et les utiliser.","metadata":{}},{"cell_type":"code","source":"# Identifions les colonnes catégorielles et Convertissons les en codes numériques\ncategorical_cols = X_train.select_dtypes(include=['category']).columns\nprint(\"Colonnes catégorielles:\", categorical_cols)\n\n# Convertir les colonnes catégorielles en catégories uniformes\nfor col in categorical_cols:\n    # Combiner les catégories des trois ensembles\n    all_categories = pd.concat([X_train[col], X_valid[col], X_test[col]], axis=0).astype('category')\n    \n    # Redéfinir les colonnes avec les catégories uniformes\n    X_train[col] = X_train[col].astype(pd.CategoricalDtype(categories=all_categories.cat.categories)).cat.codes\n    X_valid[col] = X_valid[col].astype(pd.CategoricalDtype(categories=all_categories.cat.categories)).cat.codes\n    X_test[col] = X_test[col].astype(pd.CategoricalDtype(categories=all_categories.cat.categories)).cat.codes\n\n# Conversion en float32\nX_train = X_train.astype(np.float32)\nX_valid = X_valid.astype(np.float32)\nX_test = X_test.astype(np.float32)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T13:58:38.172860Z","iopub.execute_input":"2024-05-31T13:58:38.173497Z","iopub.status.idle":"2024-05-31T13:58:38.407149Z","shell.execute_reply.started":"2024-05-31T13:58:38.173455Z","shell.execute_reply":"2024-05-31T13:58:38.405314Z"},"trusted":true},"execution_count":108,"outputs":[{"name":"stdout","text":"Colonnes catégorielles: Index([], dtype='object')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## **MODELISATION**:\n\nNous allons utiliser un modèle LightGBM qui est méthode de gradient boosting efficace pour la gestion de grands nombres de données.\\\nDans un premier temps nous allons utiliser un modèle avec des paramètres de base que nous fixerons aléatoirement. Ensuite, nous allons utliser une gridsearch, un randomized search ou une bayesian search pour optimiser notre modèle.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import ParameterGrid\n\nlgb_train = lgb.Dataset(X_train, label=y_train)\nlgb_valid = lgb.Dataset(X_valid, label=y_valid, reference=lgb_train)\n\nparams = {\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n    \"metric\": \"auc\",\n    \"max_depth\": 3,\n    \"num_leaves\": 31,\n    \"learning_rate\": 0.05,\n    \"feature_fraction\": 0.9,\n    \"bagging_fraction\": 0.8,\n    \"bagging_freq\": 5,\n    \"n_estimators\": 1000,\n    \"verbose\": -1,\n}\n\ngbm = lgb.train(\n    params,\n    lgb_train,\n    valid_sets=lgb_valid,\n    callbacks=[lgb.log_evaluation(50), lgb.early_stopping(10)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Paramètres pour une Grid Search\nparam_grid = {\n    'boosting_type': ['gbdt'],\n    'objective': ['binary'],\n    'metric': ['auc'],\n    'max_depth': [5, 7],\n    'num_leaves': [31, 63],\n    'learning_rate': [0.05, 0.1],\n    'feature_fraction': [0.8, 0.9],\n    'bagging_fraction': [0.7, 0.8],\n    'bagging_freq': [5],\n    'n_estimators': [500, 1000]\n}\n\n# Fonction pour évaluer les paramètres\ndef evaluate_params(params, X_train, y_train, X_valid, y_valid, base_valid):\n    lgb_train = lgb.Dataset(X_train, label=y_train)\n    lgb_valid = lgb.Dataset(X_valid, label=y_valid, reference=lgb_train)\n    \n    gbm = lgb.train(\n        params,\n        lgb_train,\n        valid_sets=lgb_valid,\n        callbacks=[lgb.log_evaluation(50), lgb.early_stopping(10)]\n    )\n    \n    y_pred = gbm.predict(X_valid, num_iteration=gbm.best_iteration)\n    base_valid[\"score\"] = y_pred\n    \n    return roc_auc_score(base_valid[\"target\"], base_valid[\"score\"])\n\n# Recherche des meilleurs paramètres\nbest_score = float('-inf')\nbest_params = None\n\nfor params in ParameterGrid(param_grid):\n    score = evaluate_params(params, X_train, y_train, X_valid, y_valid, base_valid)\n    if score > best_score:\n        best_score = score\n        best_params = params\n\nprint(\"Meilleurs paramètres trouvés:\")\nprint(best_params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Meilleurs paramètres trouvés:\n{'bagging_fraction': 0.7, 'bagging_freq': 5, 'boosting_type': 'gbdt', 'feature_fraction': 0.9, 'learning_rate': 0.1, 'max_depth': 5, 'metric': 'auc', 'n_estimators': 100, 'num_leaves': 31, 'objective': 'binary'}\n\n{'bagging_fraction': 0.8, 'bagging_freq': 5, 'boosting_type': 'gbdt', 'feature_fraction': 0.9, 'learning_rate': 0.05, 'max_depth': 7, 'metric': 'auc', 'n_estimators': 500, 'num_leaves': 31, 'objective': 'binary'}\n","metadata":{}},{"cell_type":"code","source":"# Entraînons le modèle final avec les meilleurs paramètres sur l'ensemble complet des données\nbest_model = lgb.LGBMClassifier(**best_params)\nbest_model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], callbacks=[lgb.early_stopping(10), lgb.log_evaluation(50)])\n\n# Prédictions et évaluation\ny_pred_train = best_model.predict_proba(X_train)[:, 1]\ny_pred_valid = best_model.predict_proba(X_valid)[:, 1]\ny_pred_test = best_model.predict_proba(X_test)[:, 1]\n\n# les scores AUC\nbase_train[\"score\"] = y_pred_train\nbase_valid[\"score\"] = y_pred_valid\nbase_test[\"score\"] = y_pred_test\n\nstability_score_train = roc_auc_score(base_train[\"target\"], base_train[\"score\"])\nstability_score_valid = roc_auc_score(base_valid[\"target\"], base_valid[\"score\"])\nstability_score_test = roc_auc_score(base_test[\"target\"], base_test[\"score\"])\n\nprint(f'The AUC score on the train set is: {stability_score_train}') \nprint(f'The AUC score on the valid set is: {stability_score_valid}') \nprint(f'The AUC score on the test set is: {stability_score_test}')","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:51:29.409742Z","iopub.execute_input":"2024-05-31T09:51:29.410178Z","iopub.status.idle":"2024-05-31T09:53:26.415812Z","shell.execute_reply.started":"2024-05-31T09:51:29.410145Z","shell.execute_reply":"2024-05-31T09:53:26.414253Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 28872, number of negative: 887123\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.544660 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8980\n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\n[50]\tvalid_0's auc: 0.699502\n[100]\tvalid_0's auc: 0.715798\n[150]\tvalid_0's auc: 0.720191\nEarly stopping, best iteration is:\n[179]\tvalid_0's auc: 0.72152\n[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\nThe AUC score on the train set is: 0.7719679254229231\nThe AUC score on the valid set is: 0.7215204701358409\nThe AUC score on the test set is: 0.7231673682933353\n","output_type":"stream"}]},{"cell_type":"code","source":"## Calcul de la métrique à considérer pour l'évaluation de notre modèle.\n\ndef gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n    gini_in_time = base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]]\\\n        .sort_values(\"WEEK_NUM\")\\\n        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]]\\\n        .apply(lambda x: 2*roc_auc_score(x[\"target\"], x[\"score\"])-1).tolist()\n    \n    x = np.arange(len(gini_in_time))\n    y = gini_in_time\n    a, b = np.polyfit(x, y, 1)\n    y_hat = a*x + b\n    residuals = y - y_hat\n    res_std = np.std(residuals)\n    avg_gini = np.mean(gini_in_time)\n    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std\n\nstability_score_train = gini_stability(base_train)\nstability_score_valid = gini_stability(base_valid)\nstability_score_test = gini_stability(base_test)\n\nprint(f'The stability score on the train set is: {stability_score_train}') \nprint(f'The stability score on the valid set is: {stability_score_valid}') \nprint(f'The stability score on the test set is: {stability_score_test}')  ","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:54:22.367134Z","iopub.execute_input":"2024-05-31T09:54:22.368193Z","iopub.status.idle":"2024-05-31T09:54:23.517045Z","shell.execute_reply.started":"2024-05-31T09:54:22.368151Z","shell.execute_reply":"2024-05-31T09:54:23.515817Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"The stability score on the train set is: 0.5185379029535647\nThe stability score on the valid set is: 0.4076619345459677\nThe stability score on the test set is: 0.4069504782503902\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"## Utilisons une BayesianSearch\n\nfrom bayes_opt import BayesianOptimization\n# Fonction d'évaluation pour la recherche bayésienne\ndef evaluate_params(max_depth, num_leaves, learning_rate, feature_fraction, bagging_fraction, bagging_freq, n_estimators):\n    params = {\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'metric': 'auc',\n        'max_depth': int(max_depth),\n        'num_leaves': int(num_leaves),\n        'learning_rate': learning_rate,\n        'feature_fraction': feature_fraction,\n        'bagging_fraction': bagging_fraction,\n        'bagging_freq': int(bagging_freq),\n        'n_estimators': int(n_estimators)\n    }\n    \n    lgb_train = lgb.Dataset(X_train, label=y_train)\n    lgb_valid = lgb.Dataset(X_valid, label=y_valid, reference=lgb_train)\n    \n    gbm = lgb.train(\n        params,\n        lgb_train,\n        valid_sets=lgb_valid,\n        callbacks=[lgb.log_evaluation(50), lgb.early_stopping(10)]\n    )\n    \n    y_pred = gbm.predict(X_valid, num_iteration=gbm.best_iteration)\n    score = roc_auc_score(base_valid[\"target\"], y_pred)\n    \n    return score\n\n# Définissons les paramètres de la recherche bayésienne\npbounds = {\n    'max_depth': (5, 7),\n    'num_leaves': (31, 63),\n    'learning_rate': (0.05, 0.1),\n    'feature_fraction': (0.8, 0.9),\n    'bagging_fraction': (0.7, 0.8),\n    'n_estimators': (100, 1000),\n    'bagging_freq': (5, 8)\n}\n\n# initialisation et optimisation bayésienne\noptimizer = BayesianOptimization(\n    f=evaluate_params,\n    pbounds=pbounds,\n    random_state=42,\n    verbose=2\n)\n\noptimizer.maximize(\n    init_points=5,\n    n_iter=10\n)\n\n# Meilleurs paramètres et score\nbest_params = optimizer.max['params']\nbest_params['max_depth'] = int(best_params['max_depth'])\nbest_params['num_leaves'] = int(best_params['num_leaves'])\nbest_params['n_estimators'] = int(best_params['n_estimators'])\nbest_score = optimizer.max['target']\n\nprint(\"Meilleurs paramètres trouvés par BayesianOptimization:\")\nprint(best_params)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T10:32:12.436026Z","iopub.execute_input":"2024-05-31T10:32:12.436514Z","iopub.status.idle":"2024-05-31T10:37:04.342172Z","shell.execute_reply.started":"2024-05-31T10:32:12.436476Z","shell.execute_reply":"2024-05-31T10:37:04.340705Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"|   iter    |  target   | baggin... | baggin... | featur... | learni... | max_depth | n_esti... | num_le... |\n-------------------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 28872, number of negative: 887123\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.346566 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8980\n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[50]\tvalid_0's auc: 0.711906\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[100]\tvalid_0's auc: 0.722648\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[104]\tvalid_0's auc: 0.722763\n| \u001b[0m1        \u001b[0m | \u001b[0m0.7228   \u001b[0m | \u001b[0m0.7375   \u001b[0m | \u001b[0m7.852    \u001b[0m | \u001b[0m0.8732   \u001b[0m | \u001b[0m0.07993  \u001b[0m | \u001b[0m5.312    \u001b[0m | \u001b[0m240.4    \u001b[0m | \u001b[0m32.86    \u001b[0m |\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 28872, number of negative: 887123\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.189341 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8980\n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\n[50]\tvalid_0's auc: 0.695119\nEarly stopping, best iteration is:\n[79]\tvalid_0's auc: 0.708467\n| \u001b[0m2        \u001b[0m | \u001b[0m0.7085   \u001b[0m | \u001b[0m0.7866   \u001b[0m | \u001b[0m6.803    \u001b[0m | \u001b[0m0.8708   \u001b[0m | \u001b[0m0.05103  \u001b[0m | \u001b[0m6.94     \u001b[0m | \u001b[0m849.2    \u001b[0m | \u001b[0m37.79    \u001b[0m |\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 28872, number of negative: 887123\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.189206 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8980\n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nTraining until validation scores don't improve for 10 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[50]\tvalid_0's auc: 0.709501\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[80]\tvalid_0's auc: 0.718499\n| \u001b[0m3        \u001b[0m | \u001b[0m0.7185   \u001b[0m | \u001b[0m0.7182   \u001b[0m | \u001b[0m5.55     \u001b[0m | \u001b[0m0.8304   \u001b[0m | \u001b[0m0.07624  \u001b[0m | \u001b[0m5.864    \u001b[0m | \u001b[0m362.1    \u001b[0m | \u001b[0m50.58    \u001b[0m |\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 28872, number of negative: 887123\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.191899 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8980\n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[50]\tvalid_0's auc: 0.703824\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[80]\tvalid_0's auc: 0.711608\n| \u001b[0m4        \u001b[0m | \u001b[0m0.7116   \u001b[0m | \u001b[0m0.7139   \u001b[0m | \u001b[0m5.876    \u001b[0m | \u001b[0m0.8366   \u001b[0m | \u001b[0m0.0728   \u001b[0m | \u001b[0m6.57     \u001b[0m | \u001b[0m279.7    \u001b[0m | \u001b[0m47.46    \u001b[0m |\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 28872, number of negative: 887123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.452196 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8980\n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[50]\tvalid_0's auc: 0.702958\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[83]\tvalid_0's auc: 0.714905\n| \u001b[0m5        \u001b[0m | \u001b[0m0.7149   \u001b[0m | \u001b[0m0.7592   \u001b[0m | \u001b[0m5.139    \u001b[0m | \u001b[0m0.8608   \u001b[0m | \u001b[0m0.05853  \u001b[0m | \u001b[0m5.13     \u001b[0m | \u001b[0m954.0    \u001b[0m | \u001b[0m61.9     \u001b[0m |\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 28872, number of negative: 887123\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.188326 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8980\n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nTraining until validation scores don't improve for 10 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[50]\tvalid_0's auc: 0.714836\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[54]\tvalid_0's auc: 0.716729\n| \u001b[0m6        \u001b[0m | \u001b[0m0.7167   \u001b[0m | \u001b[0m0.7237   \u001b[0m | \u001b[0m7.019    \u001b[0m | \u001b[0m0.883    \u001b[0m | \u001b[0m0.09897  \u001b[0m | \u001b[0m5.264    \u001b[0m | \u001b[0m240.4    \u001b[0m | \u001b[0m32.36    \u001b[0m |\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 28872, number of negative: 887123\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.190505 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8980\n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nTraining until validation scores don't improve for 10 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[50]\tvalid_0's auc: 0.712272\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[82]\tvalid_0's auc: 0.72023\n| \u001b[0m7        \u001b[0m | \u001b[0m0.7202   \u001b[0m | \u001b[0m0.7149   \u001b[0m | \u001b[0m5.744    \u001b[0m | \u001b[0m0.8774   \u001b[0m | \u001b[0m0.07222  \u001b[0m | \u001b[0m5.362    \u001b[0m | \u001b[0m361.1    \u001b[0m | \u001b[0m50.91    \u001b[0m |\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 28872, number of negative: 887123\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.312094 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8980\n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[50]\tvalid_0's auc: 0.70384\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[81]\tvalid_0's auc: 0.711596\n| \u001b[0m8        \u001b[0m | \u001b[0m0.7116   \u001b[0m | \u001b[0m0.7879   \u001b[0m | \u001b[0m5.463    \u001b[0m | \u001b[0m0.8665   \u001b[0m | \u001b[0m0.0639   \u001b[0m | \u001b[0m6.102    \u001b[0m | \u001b[0m361.3    \u001b[0m | \u001b[0m49.55    \u001b[0m |\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 28872, number of negative: 887123\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.192523 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8980\n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\n[50]\tvalid_0's auc: 0.707133\nEarly stopping, best iteration is:\n[79]\tvalid_0's auc: 0.713661\n| \u001b[0m9        \u001b[0m | \u001b[0m0.7137   \u001b[0m | \u001b[0m0.72     \u001b[0m | \u001b[0m7.838    \u001b[0m | \u001b[0m0.8157   \u001b[0m | \u001b[0m0.07432  \u001b[0m | \u001b[0m6.257    \u001b[0m | \u001b[0m240.2    \u001b[0m | \u001b[0m33.55    \u001b[0m |\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 28872, number of negative: 887123\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.190018 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8980\n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\n[50]\tvalid_0's auc: 0.702488\n[100]\tvalid_0's auc: 0.715089\nEarly stopping, best iteration is:\n[112]\tvalid_0's auc: 0.717\n| \u001b[0m10       \u001b[0m | \u001b[0m0.717    \u001b[0m | \u001b[0m0.7556   \u001b[0m | \u001b[0m7.711    \u001b[0m | \u001b[0m0.8662   \u001b[0m | \u001b[0m0.06153  \u001b[0m | \u001b[0m6.932    \u001b[0m | \u001b[0m456.5    \u001b[0m | \u001b[0m31.89    \u001b[0m |\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 28872, number of negative: 887123\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.189762 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8980\n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\n[50]\tvalid_0's auc: 0.702524\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[80]\tvalid_0's auc: 0.711023\n| \u001b[0m11       \u001b[0m | \u001b[0m0.711    \u001b[0m | \u001b[0m0.7809   \u001b[0m | \u001b[0m5.578    \u001b[0m | \u001b[0m0.8851   \u001b[0m | \u001b[0m0.08255  \u001b[0m | \u001b[0m6.326    \u001b[0m | \u001b[0m146.1    \u001b[0m | \u001b[0m44.4     \u001b[0m |\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 28872, number of negative: 887123\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.191076 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8980\n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nTraining until validation scores don't improve for 10 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[50]\tvalid_0's auc: 0.706833\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[70]\tvalid_0's auc: 0.712803\n| \u001b[0m12       \u001b[0m | \u001b[0m0.7128   \u001b[0m | \u001b[0m0.771    \u001b[0m | \u001b[0m6.894    \u001b[0m | \u001b[0m0.8802   \u001b[0m | \u001b[0m0.0837   \u001b[0m | \u001b[0m6.69     \u001b[0m | \u001b[0m294.0    \u001b[0m | \u001b[0m61.72    \u001b[0m |\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 28872, number of negative: 887123\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.187871 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8980\n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[50]\tvalid_0's auc: 0.707025\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[100]\tvalid_0's auc: 0.714688\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[112]\tvalid_0's auc: 0.717846\n| \u001b[0m13       \u001b[0m | \u001b[0m0.7178   \u001b[0m | \u001b[0m0.7119   \u001b[0m | \u001b[0m7.851    \u001b[0m | \u001b[0m0.8968   \u001b[0m | \u001b[0m0.09767  \u001b[0m | \u001b[0m6.85     \u001b[0m | \u001b[0m312.8    \u001b[0m | \u001b[0m44.49    \u001b[0m |\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 28872, number of negative: 887123\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.191993 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8980\n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nTraining until validation scores don't improve for 10 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[50]\tvalid_0's auc: 0.706197\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[100]\tvalid_0's auc: 0.71799\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[112]\tvalid_0's auc: 0.718577\n| \u001b[0m14       \u001b[0m | \u001b[0m0.7186   \u001b[0m | \u001b[0m0.7976   \u001b[0m | \u001b[0m7.1      \u001b[0m | \u001b[0m0.8903   \u001b[0m | \u001b[0m0.07223  \u001b[0m | \u001b[0m5.816    \u001b[0m | \u001b[0m316.4    \u001b[0m | \u001b[0m36.73    \u001b[0m |\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 28872, number of negative: 887123\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.189241 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8980\n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[50]\tvalid_0's auc: 0.709632\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[72]\tvalid_0's auc: 0.71719\n| \u001b[0m15       \u001b[0m | \u001b[0m0.7172   \u001b[0m | \u001b[0m0.7428   \u001b[0m | \u001b[0m5.982    \u001b[0m | \u001b[0m0.8518   \u001b[0m | \u001b[0m0.09907  \u001b[0m | \u001b[0m5.034    \u001b[0m | \u001b[0m901.5    \u001b[0m | \u001b[0m47.62    \u001b[0m |\n=============================================================================================================\nMeilleurs paramètres trouvés par BayesianOptimization:\n{'bagging_fraction': 0.7374540118847362, 'bagging_freq': 7.852142919229749, 'feature_fraction': 0.8731993941811406, 'learning_rate': 0.07993292420985183, 'max_depth': 5, 'n_estimators': 240, 'num_leaves': 32}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Meilleurs paramètres trouvés par BayesianOptimization:\n{'bagging_fraction': 0.7649629555677399, 'feature_fraction': 0.8870130839701755, 'learning_rate': 0.07796225729846441, 'max_depth': 5, 'n_estimators': 975, 'num_leaves': 48}","metadata":{}},{"cell_type":"markdown","source":"Utilisation des bests params","metadata":{}},{"cell_type":"code","source":"lgb_train = lgb.Dataset(X_train, label=y_train)\nlgb_valid = lgb.Dataset(X_valid, label=y_valid, reference=lgb_train)\n\nparams = {\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n    \"metric\": \"auc\",\n    \"max_depth\": 5,\n    \"num_leaves\": 48,\n    \"learning_rate\": 0.07796225729846441,\n    \"feature_fraction\": 0.8870130839701755, \n    \"bagging_fraction\": 0.7649629555677399,\n    \"bagging_freq\": 5,\n    \"n_estimators\": 975,\n    \"verbose\": -1,\n}\n\ngbm = lgb.train(\n    params,\n    lgb_train,\n    valid_sets=lgb_valid,\n    callbacks=[lgb.log_evaluation(60), lgb.early_stopping(10)]\n)\n\nfor base, X in [(base_train, X_train), (base_valid, X_valid), (base_test, X_test)]:\n    y_pred = gbm.predict(X, num_iteration=gbm.best_iteration)\n    base[\"score\"] = y_pred\n\nprint(f'The AUC score on the train set is: {roc_auc_score(base_train[\"target\"], base_train[\"score\"])}') \nprint(f'The AUC score on the valid set is: {roc_auc_score(base_valid[\"target\"], base_valid[\"score\"])}') \nprint(f'The AUC score on the test set is: {roc_auc_score(base_test[\"target\"], base_test[\"score\"])}')  ","metadata":{"execution":{"iopub.status.busy":"2024-05-31T15:20:46.681960Z","iopub.execute_input":"2024-05-31T15:20:46.682575Z","iopub.status.idle":"2024-05-31T15:21:19.234038Z","shell.execute_reply.started":"2024-05-31T15:20:46.682537Z","shell.execute_reply":"2024-05-31T15:21:19.232398Z"},"trusted":true},"execution_count":137,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 10 rounds\n[60]\tvalid_0's auc: 0.713083\n[120]\tvalid_0's auc: 0.721541\nEarly stopping, best iteration is:\n[122]\tvalid_0's auc: 0.721824\nThe AUC score on the train set is: 0.7672716241168197\nThe AUC score on the valid set is: 0.7218236929890642\nThe AUC score on the test set is: 0.72278874702814\n","output_type":"stream"}]},{"cell_type":"code","source":"def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n    gini_in_time = base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]]\\\n        .sort_values(\"WEEK_NUM\")\\\n        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]]\\\n        .apply(lambda x: 2*roc_auc_score(x[\"target\"], x[\"score\"])-1).tolist()\n    \n    x = np.arange(len(gini_in_time))\n    y = gini_in_time\n    a, b = np.polyfit(x, y, 1)\n    y_hat = a*x + b\n    residuals = y - y_hat\n    res_std = np.std(residuals)\n    avg_gini = np.mean(gini_in_time)\n    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std\n\nstability_score_train = gini_stability(base_train)\nstability_score_valid = gini_stability(base_valid)\nstability_score_test = gini_stability(base_test)\n\nprint(f'The stability score on the train set is: {stability_score_train}') \nprint(f'The stability score on the valid set is: {stability_score_valid}') \nprint(f'The stability score on the test set is: {stability_score_test}')  ","metadata":{"execution":{"iopub.status.busy":"2024-05-31T15:21:31.521274Z","iopub.execute_input":"2024-05-31T15:21:31.521848Z","iopub.status.idle":"2024-05-31T15:21:32.876905Z","shell.execute_reply.started":"2024-05-31T15:21:31.521808Z","shell.execute_reply":"2024-05-31T15:21:32.875252Z"},"trusted":true},"execution_count":138,"outputs":[{"name":"stdout","text":"The stability score on the train set is: 0.5090086963195201\nThe stability score on the valid set is: 0.40621974237788144\nThe stability score on the test set is: 0.4032130762785661\n","output_type":"stream"}]},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"### randomized search\n\nfrom scipy.stats import randint, uniform\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Paramètres pour Randomized Search\nparam_dist = {\n    'boosting_type': ['gbdt'],\n    'objective': ['binary'],\n    'metric': ['auc'],\n    'max_depth': randint(5, 8),\n    'num_leaves': randint(30, 90),\n    'learning_rate': uniform(0.05, 0.9),\n    'feature_fraction': uniform(0.8, 0.2),\n    'bagging_fraction': uniform(0.01, 0.9),\n    'bagging_freq': [5],\n    'n_estimators': randint(100, 1001)\n}\n\n# Modèle LightGBM\nlgb_model = lgb.LGBMClassifier()\n\n# Randomized Search\nrandom_search = RandomizedSearchCV(\n    estimator=lgb_model,\n    param_distributions=param_dist,\n    n_iter=10,\n    scoring='roc_auc',\n    cv=3,\n    verbose=1,\n    random_state=42,\n    n_jobs=-1\n)\n\n# Entraîner la recherche de paramètres\nrandom_search.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], eval_metric='auc', callbacks=[lgb.early_stopping(10), lgb.log_evaluation(50)])\n\n# Meilleurs paramètres et score\nbest_params = random_search.best_params_\nbest_score = random_search.best_score_\n\nprint(\"Meilleurs paramètres trouvés par RandomizedSearchCV:\")\nprint(best_params)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T10:43:49.855112Z","iopub.execute_input":"2024-05-31T10:43:49.855690Z","iopub.status.idle":"2024-05-31T10:51:44.655321Z","shell.execute_reply.started":"2024-05-31T10:43:49.855636Z","shell.execute_reply":"2024-05-31T10:51:44.653893Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Fitting 3 folds for each of 10 candidates, totalling 30 fits\n[LightGBM] [Warning] feature_fraction is set=0.9901428612819833, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9901428612819833\n[LightGBM] [Warning] bagging_fraction is set=0.7374540118847362, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7374540118847362\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.9901428612819833, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9901428612819833\n[LightGBM] [Warning] bagging_fraction is set=0.7374540118847362, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7374540118847362\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591415\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.371937 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 9012\n[LightGBM] [Info] Number of data points in the train set: 610663, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.9901428612819833, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9901428612819833\n[LightGBM] [Warning] bagging_fraction is set=0.7374540118847362, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7374540118847362\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[12]\tvalid_0's auc: 0.678975\n[LightGBM] [Warning] feature_fraction is set=0.9901428612819833, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9901428612819833\n[LightGBM] [Warning] bagging_fraction is set=0.7374540118847362, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7374540118847362\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.9901428612819833, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9901428612819833\n[LightGBM] [Warning] bagging_fraction is set=0.7374540118847362, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7374540118847362\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.9901428612819833, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9901428612819833\n[LightGBM] [Warning] bagging_fraction is set=0.7374540118847362, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7374540118847362\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591415\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.351865 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 9000\n[LightGBM] [Info] Number of data points in the train set: 610663, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.9901428612819833, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9901428612819833\n[LightGBM] [Warning] bagging_fraction is set=0.7374540118847362, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7374540118847362\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[15]\tvalid_0's auc: 0.661695\n[LightGBM] [Warning] feature_fraction is set=0.9901428612819833, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9901428612819833\n[LightGBM] [Warning] bagging_fraction is set=0.7374540118847362, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7374540118847362\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.9901428612819833, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9901428612819833\n[LightGBM] [Warning] bagging_fraction is set=0.7374540118847362, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7374540118847362\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.9901428612819833, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9901428612819833\n[LightGBM] [Warning] bagging_fraction is set=0.7374540118847362, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7374540118847362\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591416\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.324315 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8654\n[LightGBM] [Info] Number of data points in the train set: 610664, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.9901428612819833, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9901428612819833\n[LightGBM] [Warning] bagging_fraction is set=0.7374540118847362, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7374540118847362\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425113\n[LightGBM] [Info] Start training from score -3.425113\nTraining until validation scores don't improve for 10 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[16]\tvalid_0's auc: 0.66434\n[LightGBM] [Warning] feature_fraction is set=0.9901428612819833, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9901428612819833\n[LightGBM] [Warning] bagging_fraction is set=0.7374540118847362, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7374540118847362\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8199949831636006, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8199949831636006\n[LightGBM] [Warning] bagging_fraction is set=0.7445832752853591, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7445832752853591\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8199949831636006, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8199949831636006\n[LightGBM] [Warning] bagging_fraction is set=0.7445832752853591, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7445832752853591\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591415\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.360266 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 9012\n[LightGBM] [Info] Number of data points in the train set: 610663, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.8199949831636006, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8199949831636006\n[LightGBM] [Warning] bagging_fraction is set=0.7445832752853591, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7445832752853591\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[19]\tvalid_0's auc: 0.689087\n[LightGBM] [Warning] feature_fraction is set=0.8199949831636006, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8199949831636006\n[LightGBM] [Warning] bagging_fraction is set=0.7445832752853591, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7445832752853591\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8199949831636006, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8199949831636006\n[LightGBM] [Warning] bagging_fraction is set=0.7445832752853591, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7445832752853591\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8199949831636006, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8199949831636006\n[LightGBM] [Warning] bagging_fraction is set=0.7445832752853591, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7445832752853591\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591415\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.341466 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 9000\n[LightGBM] [Info] Number of data points in the train set: 610663, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.8199949831636006, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8199949831636006\n[LightGBM] [Warning] bagging_fraction is set=0.7445832752853591, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7445832752853591\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[12]\tvalid_0's auc: 0.680019\n[LightGBM] [Warning] feature_fraction is set=0.8199949831636006, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8199949831636006\n[LightGBM] [Warning] bagging_fraction is set=0.7445832752853591, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7445832752853591\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8199949831636006, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8199949831636006\n[LightGBM] [Warning] bagging_fraction is set=0.7445832752853591, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7445832752853591\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8199949831636006, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8199949831636006\n[LightGBM] [Warning] bagging_fraction is set=0.7445832752853591, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7445832752853591\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591416\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.565695 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8654\n[LightGBM] [Info] Number of data points in the train set: 610664, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.8199949831636006, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8199949831636006\n[LightGBM] [Warning] bagging_fraction is set=0.7445832752853591, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7445832752853591\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425113\n[LightGBM] [Info] Start training from score -3.425113\nTraining until validation scores don't improve for 10 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[26]\tvalid_0's auc: 0.692308\n[LightGBM] [Warning] feature_fraction is set=0.8199949831636006, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8199949831636006\n[LightGBM] [Warning] bagging_fraction is set=0.7445832752853591, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7445832752853591\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8041168988591605, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8041168988591605\n[LightGBM] [Warning] bagging_fraction is set=0.7708072577796045, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7708072577796045\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8041168988591605, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8041168988591605\n[LightGBM] [Warning] bagging_fraction is set=0.7708072577796045, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7708072577796045\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591415\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.361489 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 9012\n[LightGBM] [Info] Number of data points in the train set: 610663, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.8041168988591605, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8041168988591605\n[LightGBM] [Warning] bagging_fraction is set=0.7708072577796045, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7708072577796045\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[17]\tvalid_0's auc: 0.658379\n[LightGBM] [Warning] feature_fraction is set=0.8041168988591605, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8041168988591605\n[LightGBM] [Warning] bagging_fraction is set=0.7708072577796045, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7708072577796045\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8041168988591605, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8041168988591605\n[LightGBM] [Warning] bagging_fraction is set=0.7708072577796045, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7708072577796045\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8041168988591605, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8041168988591605\n[LightGBM] [Warning] bagging_fraction is set=0.7708072577796045, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7708072577796045\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591415\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.341313 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 9000\n[LightGBM] [Info] Number of data points in the train set: 610663, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.8041168988591605, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8041168988591605\n[LightGBM] [Warning] bagging_fraction is set=0.7708072577796045, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7708072577796045\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[32]\tvalid_0's auc: 0.642751\n[LightGBM] [Warning] feature_fraction is set=0.8041168988591605, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8041168988591605\n[LightGBM] [Warning] bagging_fraction is set=0.7708072577796045, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7708072577796045\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8041168988591605, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8041168988591605\n[LightGBM] [Warning] bagging_fraction is set=0.7708072577796045, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7708072577796045\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8041168988591605, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8041168988591605\n[LightGBM] [Warning] bagging_fraction is set=0.7708072577796045, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7708072577796045\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591416\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.319182 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8654\n[LightGBM] [Info] Number of data points in the train set: 610664, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.8041168988591605, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8041168988591605\n[LightGBM] [Warning] bagging_fraction is set=0.7708072577796045, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7708072577796045\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425113\n[LightGBM] [Info] Start training from score -3.425113\nTraining until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[13]\tvalid_0's auc: 0.647198\n[LightGBM] [Warning] feature_fraction is set=0.8041168988591605, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8041168988591605\n[LightGBM] [Warning] bagging_fraction is set=0.7708072577796045, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7708072577796045\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8366809019706868, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8366809019706868\n[LightGBM] [Warning] bagging_fraction is set=0.71818249672071, subsample=1.0 will be ignored. Current value: bagging_fraction=0.71818249672071\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] feature_fraction is set=0.8366809019706868, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8366809019706868\n[LightGBM] [Warning] bagging_fraction is set=0.71818249672071, subsample=1.0 will be ignored. Current value: bagging_fraction=0.71818249672071\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591415\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.363735 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 9012\n[LightGBM] [Info] Number of data points in the train set: 610663, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.8366809019706868, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8366809019706868\n[LightGBM] [Warning] bagging_fraction is set=0.71818249672071, subsample=1.0 will be ignored. Current value: bagging_fraction=0.71818249672071\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[19]\tvalid_0's auc: 0.70856\n[LightGBM] [Warning] feature_fraction is set=0.8366809019706868, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8366809019706868\n[LightGBM] [Warning] bagging_fraction is set=0.71818249672071, subsample=1.0 will be ignored. Current value: bagging_fraction=0.71818249672071\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] feature_fraction is set=0.8366809019706868, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8366809019706868\n[LightGBM] [Warning] bagging_fraction is set=0.71818249672071, subsample=1.0 will be ignored. Current value: bagging_fraction=0.71818249672071\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] feature_fraction is set=0.8366809019706868, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8366809019706868\n[LightGBM] [Warning] bagging_fraction is set=0.71818249672071, subsample=1.0 will be ignored. Current value: bagging_fraction=0.71818249672071\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591415\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.347273 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 9000\n[LightGBM] [Info] Number of data points in the train set: 610663, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.8366809019706868, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8366809019706868\n[LightGBM] [Warning] bagging_fraction is set=0.71818249672071, subsample=1.0 will be ignored. Current value: bagging_fraction=0.71818249672071\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\n[50]\tvalid_0's auc: 0.703085\nEarly stopping, best iteration is:\n[43]\tvalid_0's auc: 0.705994\n[LightGBM] [Warning] feature_fraction is set=0.8366809019706868, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8366809019706868\n[LightGBM] [Warning] bagging_fraction is set=0.71818249672071, subsample=1.0 will be ignored. Current value: bagging_fraction=0.71818249672071\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] feature_fraction is set=0.8366809019706868, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8366809019706868\n[LightGBM] [Warning] bagging_fraction is set=0.71818249672071, subsample=1.0 will be ignored. Current value: bagging_fraction=0.71818249672071\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] feature_fraction is set=0.8366809019706868, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8366809019706868\n[LightGBM] [Warning] bagging_fraction is set=0.71818249672071, subsample=1.0 will be ignored. Current value: bagging_fraction=0.71818249672071\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591416\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.297024 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8654\n[LightGBM] [Info] Number of data points in the train set: 610664, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.8366809019706868, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8366809019706868\n[LightGBM] [Warning] bagging_fraction is set=0.71818249672071, subsample=1.0 will be ignored. Current value: bagging_fraction=0.71818249672071\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425113\n[LightGBM] [Info] Start training from score -3.425113\nTraining until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[12]\tvalid_0's auc: 0.685751\n[LightGBM] [Warning] feature_fraction is set=0.8366809019706868, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8366809019706868\n[LightGBM] [Warning] bagging_fraction is set=0.71818249672071, subsample=1.0 will be ignored. Current value: bagging_fraction=0.71818249672071\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] feature_fraction is set=0.9049549320516779, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9049549320516779\n[LightGBM] [Warning] bagging_fraction is set=0.7023062425041415, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7023062425041415\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.9049549320516779, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9049549320516779\n[LightGBM] [Warning] bagging_fraction is set=0.7023062425041415, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7023062425041415\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591415\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.367165 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 9012\n[LightGBM] [Info] Number of data points in the train set: 610663, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.9049549320516779, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9049549320516779\n[LightGBM] [Warning] bagging_fraction is set=0.7023062425041415, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7023062425041415\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[19]\tvalid_0's auc: 0.692481\n[LightGBM] [Warning] feature_fraction is set=0.9049549320516779, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9049549320516779\n[LightGBM] [Warning] bagging_fraction is set=0.7023062425041415, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7023062425041415\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.9049549320516779, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9049549320516779\n[LightGBM] [Warning] bagging_fraction is set=0.7023062425041415, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7023062425041415\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.9049549320516779, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9049549320516779\n[LightGBM] [Warning] bagging_fraction is set=0.7023062425041415, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7023062425041415\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591415\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.345889 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 9000\n[LightGBM] [Info] Number of data points in the train set: 610663, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.9049549320516779, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9049549320516779\n[LightGBM] [Warning] bagging_fraction is set=0.7023062425041415, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7023062425041415\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[20]\tvalid_0's auc: 0.689188\n[LightGBM] [Warning] feature_fraction is set=0.9049549320516779, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9049549320516779\n[LightGBM] [Warning] bagging_fraction is set=0.7023062425041415, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7023062425041415\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.9049549320516779, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9049549320516779\n[LightGBM] [Warning] bagging_fraction is set=0.7023062425041415, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7023062425041415\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.9049549320516779, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9049549320516779\n[LightGBM] [Warning] bagging_fraction is set=0.7023062425041415, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7023062425041415\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591416\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.308988 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8654\n[LightGBM] [Info] Number of data points in the train set: 610664, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.9049549320516779, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9049549320516779\n[LightGBM] [Warning] bagging_fraction is set=0.7023062425041415, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7023062425041415\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425113\n[LightGBM] [Info] Start training from score -3.425113\nTraining until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[15]\tvalid_0's auc: 0.681422\n[LightGBM] [Warning] feature_fraction is set=0.9049549320516779, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9049549320516779\n[LightGBM] [Warning] bagging_fraction is set=0.7023062425041415, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7023062425041415\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8764923982534326, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8764923982534326\n[LightGBM] [Warning] bagging_fraction is set=0.7618386009333087, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7618386009333087\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8764923982534326, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8764923982534326\n[LightGBM] [Warning] bagging_fraction is set=0.7618386009333087, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7618386009333087\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591415\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.365797 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 9012\n[LightGBM] [Info] Number of data points in the train set: 610663, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.8764923982534326, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8764923982534326\n[LightGBM] [Warning] bagging_fraction is set=0.7618386009333087, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7618386009333087\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[8]\tvalid_0's auc: 0.660199\n[LightGBM] [Warning] feature_fraction is set=0.8764923982534326, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8764923982534326\n[LightGBM] [Warning] bagging_fraction is set=0.7618386009333087, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7618386009333087\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8764923982534326, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8764923982534326\n[LightGBM] [Warning] bagging_fraction is set=0.7618386009333087, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7618386009333087\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8764923982534326, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8764923982534326\n[LightGBM] [Warning] bagging_fraction is set=0.7618386009333087, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7618386009333087\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591415\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.345490 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 9000\n[LightGBM] [Info] Number of data points in the train set: 610663, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.8764923982534326, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8764923982534326\n[LightGBM] [Warning] bagging_fraction is set=0.7618386009333087, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7618386009333087\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[21]\tvalid_0's auc: 0.656374\n[LightGBM] [Warning] feature_fraction is set=0.8764923982534326, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8764923982534326\n[LightGBM] [Warning] bagging_fraction is set=0.7618386009333087, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7618386009333087\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8764923982534326, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8764923982534326\n[LightGBM] [Warning] bagging_fraction is set=0.7618386009333087, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7618386009333087\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8764923982534326, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8764923982534326\n[LightGBM] [Warning] bagging_fraction is set=0.7618386009333087, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7618386009333087\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591416\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.300422 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8654\n[LightGBM] [Info] Number of data points in the train set: 610664, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.8764923982534326, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8764923982534326\n[LightGBM] [Warning] bagging_fraction is set=0.7618386009333087, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7618386009333087\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425113\n[LightGBM] [Info] Start training from score -3.425113\nTraining until validation scores don't improve for 10 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[4]\tvalid_0's auc: 0.641161\n[LightGBM] [Warning] feature_fraction is set=0.8764923982534326, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8764923982534326\n[LightGBM] [Warning] bagging_fraction is set=0.7618386009333087, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7618386009333087\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8341048247374584, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8341048247374584\n[LightGBM] [Warning] bagging_fraction is set=0.7607544851901438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7607544851901438\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8341048247374584, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8341048247374584\n[LightGBM] [Warning] bagging_fraction is set=0.7607544851901438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7607544851901438\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591415\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.363010 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 9012\n[LightGBM] [Info] Number of data points in the train set: 610663, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.8341048247374584, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8341048247374584\n[LightGBM] [Warning] bagging_fraction is set=0.7607544851901438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7607544851901438\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[50]\tvalid_0's auc: 0.711921\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[49]\tvalid_0's auc: 0.712198\n[LightGBM] [Warning] feature_fraction is set=0.8341048247374584, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8341048247374584\n[LightGBM] [Warning] bagging_fraction is set=0.7607544851901438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7607544851901438\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8341048247374584, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8341048247374584\n[LightGBM] [Warning] bagging_fraction is set=0.7607544851901438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7607544851901438\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8341048247374584, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8341048247374584\n[LightGBM] [Warning] bagging_fraction is set=0.7607544851901438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7607544851901438\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591415\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.343185 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 9000\n[LightGBM] [Info] Number of data points in the train set: 610663, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.8341048247374584, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8341048247374584\n[LightGBM] [Warning] bagging_fraction is set=0.7607544851901438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7607544851901438\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[35]\tvalid_0's auc: 0.695002\n[LightGBM] [Warning] feature_fraction is set=0.8341048247374584, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8341048247374584\n[LightGBM] [Warning] bagging_fraction is set=0.7607544851901438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7607544851901438\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8341048247374584, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8341048247374584\n[LightGBM] [Warning] bagging_fraction is set=0.7607544851901438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7607544851901438\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8341048247374584, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8341048247374584\n[LightGBM] [Warning] bagging_fraction is set=0.7607544851901438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7607544851901438\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591416\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.299981 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8654\n[LightGBM] [Info] Number of data points in the train set: 610664, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.8341048247374584, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8341048247374584\n[LightGBM] [Warning] bagging_fraction is set=0.7607544851901438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7607544851901438\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425113\n[LightGBM] [Info] Start training from score -3.425113\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nTraining until validation scores don't improve for 10 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[50]\tvalid_0's auc: 0.696276\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[78]\tvalid_0's auc: 0.70425\n[LightGBM] [Warning] feature_fraction is set=0.8341048247374584, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8341048247374584\n[LightGBM] [Warning] bagging_fraction is set=0.7607544851901438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7607544851901438\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8609227538346742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8609227538346742\n[LightGBM] [Warning] bagging_fraction is set=0.780839734811646, subsample=1.0 will be ignored. Current value: bagging_fraction=0.780839734811646\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] feature_fraction is set=0.8609227538346742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8609227538346742\n[LightGBM] [Warning] bagging_fraction is set=0.780839734811646, subsample=1.0 will be ignored. Current value: bagging_fraction=0.780839734811646\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591415\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.365504 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 9012\n[LightGBM] [Info] Number of data points in the train set: 610663, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.8609227538346742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8609227538346742\n[LightGBM] [Warning] bagging_fraction is set=0.780839734811646, subsample=1.0 will be ignored. Current value: bagging_fraction=0.780839734811646\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[35]\tvalid_0's auc: 0.724708\n[LightGBM] [Warning] feature_fraction is set=0.8609227538346742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8609227538346742\n[LightGBM] [Warning] bagging_fraction is set=0.780839734811646, subsample=1.0 will be ignored. Current value: bagging_fraction=0.780839734811646\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] feature_fraction is set=0.8609227538346742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8609227538346742\n[LightGBM] [Warning] bagging_fraction is set=0.780839734811646, subsample=1.0 will be ignored. Current value: bagging_fraction=0.780839734811646\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] feature_fraction is set=0.8609227538346742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8609227538346742\n[LightGBM] [Warning] bagging_fraction is set=0.780839734811646, subsample=1.0 will be ignored. Current value: bagging_fraction=0.780839734811646\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591415\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.341440 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 9000\n[LightGBM] [Info] Number of data points in the train set: 610663, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.8609227538346742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8609227538346742\n[LightGBM] [Warning] bagging_fraction is set=0.780839734811646, subsample=1.0 will be ignored. Current value: bagging_fraction=0.780839734811646\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[35]\tvalid_0's auc: 0.706014\n[LightGBM] [Warning] feature_fraction is set=0.8609227538346742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8609227538346742\n[LightGBM] [Warning] bagging_fraction is set=0.780839734811646, subsample=1.0 will be ignored. Current value: bagging_fraction=0.780839734811646\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] feature_fraction is set=0.8609227538346742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8609227538346742\n[LightGBM] [Warning] bagging_fraction is set=0.780839734811646, subsample=1.0 will be ignored. Current value: bagging_fraction=0.780839734811646\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] feature_fraction is set=0.8609227538346742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8609227538346742\n[LightGBM] [Warning] bagging_fraction is set=0.780839734811646, subsample=1.0 will be ignored. Current value: bagging_fraction=0.780839734811646\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591416\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.313764 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8654\n[LightGBM] [Info] Number of data points in the train set: 610664, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.8609227538346742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8609227538346742\n[LightGBM] [Warning] bagging_fraction is set=0.780839734811646, subsample=1.0 will be ignored. Current value: bagging_fraction=0.780839734811646\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425113\n[LightGBM] [Info] Start training from score -3.425113\nTraining until validation scores don't improve for 10 rounds\n[50]\tvalid_0's auc: 0.702576\nEarly stopping, best iteration is:\n[54]\tvalid_0's auc: 0.702809\n[LightGBM] [Warning] feature_fraction is set=0.8609227538346742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8609227538346742\n[LightGBM] [Warning] bagging_fraction is set=0.780839734811646, subsample=1.0 will be ignored. Current value: bagging_fraction=0.780839734811646\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] feature_fraction is set=0.8068777042230437, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8068777042230437\n[LightGBM] [Warning] bagging_fraction is set=0.749517691011127, subsample=1.0 will be ignored. Current value: bagging_fraction=0.749517691011127\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8068777042230437, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8068777042230437\n[LightGBM] [Warning] bagging_fraction is set=0.749517691011127, subsample=1.0 will be ignored. Current value: bagging_fraction=0.749517691011127\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591415\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.362542 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 9012\n[LightGBM] [Info] Number of data points in the train set: 610663, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.8068777042230437, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8068777042230437\n[LightGBM] [Warning] bagging_fraction is set=0.749517691011127, subsample=1.0 will be ignored. Current value: bagging_fraction=0.749517691011127\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[9]\tvalid_0's auc: 0.668315\n[LightGBM] [Warning] feature_fraction is set=0.8068777042230437, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8068777042230437\n[LightGBM] [Warning] bagging_fraction is set=0.749517691011127, subsample=1.0 will be ignored. Current value: bagging_fraction=0.749517691011127\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8068777042230437, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8068777042230437\n[LightGBM] [Warning] bagging_fraction is set=0.749517691011127, subsample=1.0 will be ignored. Current value: bagging_fraction=0.749517691011127\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8068777042230437, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8068777042230437\n[LightGBM] [Warning] bagging_fraction is set=0.749517691011127, subsample=1.0 will be ignored. Current value: bagging_fraction=0.749517691011127\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591415\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.341004 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 9000\n[LightGBM] [Info] Number of data points in the train set: 610663, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.8068777042230437, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8068777042230437\n[LightGBM] [Warning] bagging_fraction is set=0.749517691011127, subsample=1.0 will be ignored. Current value: bagging_fraction=0.749517691011127\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[14]\tvalid_0's auc: 0.623579\n[LightGBM] [Warning] feature_fraction is set=0.8068777042230437, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8068777042230437\n[LightGBM] [Warning] bagging_fraction is set=0.749517691011127, subsample=1.0 will be ignored. Current value: bagging_fraction=0.749517691011127\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8068777042230437, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8068777042230437\n[LightGBM] [Warning] bagging_fraction is set=0.749517691011127, subsample=1.0 will be ignored. Current value: bagging_fraction=0.749517691011127\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8068777042230437, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8068777042230437\n[LightGBM] [Warning] bagging_fraction is set=0.749517691011127, subsample=1.0 will be ignored. Current value: bagging_fraction=0.749517691011127\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591416\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.299003 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8654\n[LightGBM] [Info] Number of data points in the train set: 610664, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.8068777042230437, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8068777042230437\n[LightGBM] [Warning] bagging_fraction is set=0.749517691011127, subsample=1.0 will be ignored. Current value: bagging_fraction=0.749517691011127\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425113\n[LightGBM] [Info] Start training from score -3.425113\nTraining until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[11]\tvalid_0's auc: 0.657076\n[LightGBM] [Warning] feature_fraction is set=0.8068777042230437, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8068777042230437\n[LightGBM] [Warning] bagging_fraction is set=0.749517691011127, subsample=1.0 will be ignored. Current value: bagging_fraction=0.749517691011127\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.9040136042355622, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9040136042355622\n[LightGBM] [Warning] bagging_fraction is set=0.7311711076089411, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7311711076089411\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.9040136042355622, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9040136042355622\n[LightGBM] [Warning] bagging_fraction is set=0.7311711076089411, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7311711076089411\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591415\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.367357 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 9012\n[LightGBM] [Info] Number of data points in the train set: 610663, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.9040136042355622, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9040136042355622\n[LightGBM] [Warning] bagging_fraction is set=0.7311711076089411, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7311711076089411\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[22]\tvalid_0's auc: 0.689774\n[LightGBM] [Warning] feature_fraction is set=0.9040136042355622, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9040136042355622\n[LightGBM] [Warning] bagging_fraction is set=0.7311711076089411, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7311711076089411\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.9040136042355622, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9040136042355622\n[LightGBM] [Warning] bagging_fraction is set=0.7311711076089411, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7311711076089411\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.9040136042355622, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9040136042355622\n[LightGBM] [Warning] bagging_fraction is set=0.7311711076089411, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7311711076089411\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591415\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.347911 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 9000\n[LightGBM] [Info] Number of data points in the train set: 610663, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.9040136042355622, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9040136042355622\n[LightGBM] [Warning] bagging_fraction is set=0.7311711076089411, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7311711076089411\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[23]\tvalid_0's auc: 0.669574\n[LightGBM] [Warning] feature_fraction is set=0.9040136042355622, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9040136042355622\n[LightGBM] [Warning] bagging_fraction is set=0.7311711076089411, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7311711076089411\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.9040136042355622, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9040136042355622\n[LightGBM] [Warning] bagging_fraction is set=0.7311711076089411, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7311711076089411\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.9040136042355622, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9040136042355622\n[LightGBM] [Warning] bagging_fraction is set=0.7311711076089411, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7311711076089411\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] Number of positive: 19248, number of negative: 591416\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.301212 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8654\n[LightGBM] [Info] Number of data points in the train set: 610664, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.9040136042355622, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9040136042355622\n[LightGBM] [Warning] bagging_fraction is set=0.7311711076089411, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7311711076089411\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425113\n[LightGBM] [Info] Start training from score -3.425113\nTraining until validation scores don't improve for 10 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[13]\tvalid_0's auc: 0.67313\n[LightGBM] [Warning] feature_fraction is set=0.9040136042355622, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9040136042355622\n[LightGBM] [Warning] bagging_fraction is set=0.7311711076089411, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7311711076089411\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8609227538346742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8609227538346742\n[LightGBM] [Warning] bagging_fraction is set=0.780839734811646, subsample=1.0 will be ignored. Current value: bagging_fraction=0.780839734811646\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] feature_fraction is set=0.8609227538346742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8609227538346742\n[LightGBM] [Warning] bagging_fraction is set=0.780839734811646, subsample=1.0 will be ignored. Current value: bagging_fraction=0.780839734811646\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 28872, number of negative: 887123\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.539832 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8980\n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Warning] feature_fraction is set=0.8609227538346742, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8609227538346742\n[LightGBM] [Warning] bagging_fraction is set=0.780839734811646, subsample=1.0 will be ignored. Current value: bagging_fraction=0.780839734811646\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[35]\tvalid_0's auc: 0.72252\nMeilleurs paramètres trouvés par RandomizedSearchCV:\n{'bagging_fraction': 0.780839734811646, 'bagging_freq': 5, 'boosting_type': 'gbdt', 'feature_fraction': 0.8609227538346742, 'learning_rate': 0.1379049026057455, 'max_depth': 7, 'metric': 'auc', 'n_estimators': 554, 'num_leaves': 31, 'objective': 'binary'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Meilleurs paramètres trouvés par RandomizedSearchCV:\n{'bagging_fraction': 0.780839734811646, 'bagging_freq': 5, 'boosting_type': 'gbdt', 'feature_fraction': 0.8609227538346742, 'learning_rate': 0.1379049026057455, 'max_depth': 7, 'metric': 'auc', 'n_estimators': 554, 'num_leaves': 31, 'objective': 'binary'}","metadata":{}},{"cell_type":"code","source":"lgb_train = lgb.Dataset(X_train, label=y_train)\nlgb_valid = lgb.Dataset(X_valid, label=y_valid, reference=lgb_train)\n\nparams = {\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n    \"metric\": \"auc\",\n    \"max_depth\": 7,\n    \"num_leaves\": 31,\n    \"learning_rate\": 0.1379049026057455,\n    \"feature_fraction\": 0.8609227538346742, \n    \"bagging_fraction\": 0.780839734811646,\n    \"bagging_freq\": 5,\n    \"n_estimators\": 554,\n    \"verbose\": -1,\n}\n\ngbm = lgb.train(\n    params,\n    lgb_train,\n    valid_sets=lgb_valid,\n    callbacks=[lgb.log_evaluation(60), lgb.early_stopping(10)]\n)\n\nfor base, X in [(base_train, X_train), (base_valid, X_valid), (base_test, X_test)]:\n    y_pred = gbm.predict(X, num_iteration=gbm.best_iteration)\n    base[\"score\"] = y_pred\n\nprint(f'The AUC score on the train set is: {roc_auc_score(base_train[\"target\"], base_train[\"score\"])}') \nprint(f'The AUC score on the valid set is: {roc_auc_score(base_valid[\"target\"], base_valid[\"score\"])}') \nprint(f'The AUC score on the test set is: {roc_auc_score(base_test[\"target\"], base_test[\"score\"])}')","metadata":{"execution":{"iopub.status.busy":"2024-05-31T12:48:12.523513Z","iopub.execute_input":"2024-05-31T12:48:12.524184Z","iopub.status.idle":"2024-05-31T12:48:30.605639Z","shell.execute_reply.started":"2024-05-31T12:48:12.524140Z","shell.execute_reply":"2024-05-31T12:48:30.603823Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 28872, number of negative: 887123\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.213033 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8980\n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111\nTraining until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[35]\tvalid_0's auc: 0.72252\nThe AUC score on the train set is: 0.7541449399822275\nThe AUC score on the valid set is: 0.7225202908527186\nThe AUC score on the test set is: 0.7212562105697082\n","output_type":"stream"}]},{"cell_type":"code","source":"def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n    gini_in_time = base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]]\\\n        .sort_values(\"WEEK_NUM\")\\\n        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]]\\\n        .apply(lambda x: 2*roc_auc_score(x[\"target\"], x[\"score\"])-1).tolist()\n    \n    x = np.arange(len(gini_in_time))\n    y = gini_in_time\n    a, b = np.polyfit(x, y, 1)\n    y_hat = a*x + b\n    residuals = y - y_hat\n    res_std = np.std(residuals)\n    avg_gini = np.mean(gini_in_time)\n    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std\n\nstability_score_train = gini_stability(base_train)\nstability_score_valid = gini_stability(base_valid)\nstability_score_test = gini_stability(base_test)\n\nprint(f'The stability score on the train set is: {stability_score_train}') \nprint(f'The stability score on the valid set is: {stability_score_valid}') \nprint(f'The stability score on the test set is: {stability_score_test}')  ","metadata":{"execution":{"iopub.status.busy":"2024-05-31T12:48:42.675947Z","iopub.execute_input":"2024-05-31T12:48:42.676507Z","iopub.status.idle":"2024-05-31T12:48:44.279714Z","shell.execute_reply.started":"2024-05-31T12:48:42.676466Z","shell.execute_reply":"2024-05-31T12:48:44.277929Z"},"trusted":true},"execution_count":95,"outputs":[{"name":"stdout","text":"The stability score on the train set is: 0.47920748162231436\nThe stability score on the valid set is: 0.4109371166357929\nThe stability score on the test set is: 0.40613899443705753\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install hyperopt","metadata":{"execution":{"iopub.status.busy":"2024-05-31T10:57:11.259162Z","iopub.execute_input":"2024-05-31T10:57:11.260120Z","iopub.status.idle":"2024-05-31T10:57:27.087619Z","shell.execute_reply.started":"2024-05-31T10:57:11.260086Z","shell.execute_reply":"2024-05-31T10:57:27.085855Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"Requirement already satisfied: hyperopt in /opt/conda/lib/python3.10/site-packages (0.2.7)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from hyperopt) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from hyperopt) (1.11.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from hyperopt) (1.16.0)\nRequirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.10/site-packages (from hyperopt) (3.2.1)\nRequirement already satisfied: future in /opt/conda/lib/python3.10/site-packages (from hyperopt) (1.0.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from hyperopt) (4.66.1)\nRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from hyperopt) (2.2.1)\nRequirement already satisfied: py4j in /opt/conda/lib/python3.10/site-packages (from hyperopt) (0.10.9.7)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"##autre méthode d'optimisation:\n\nfrom hyperopt import hp, tpe, fmin, Trials, STATUS_OK\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\n\ndef evaluate_params(params):\n    params['max_depth'] = int(params['max_depth'])\n    params['num_leaves'] = int(params['num_leaves'])\n\n    lgb_train = lgb.Dataset(X_train, label=y_train)\n    lgb_valid = lgb.Dataset(X_valid, label=y_valid, reference=lgb_train)\n\n    gbm = lgb.train(\n        params,\n        lgb_train,\n        valid_sets=lgb_valid,\n        callbacks=[lgb.log_evaluation(100), lgb.early_stopping(10)]\n    )\n\n    y_pred = gbm.predict(X_valid, num_iteration=gbm.best_iteration)\n    score = roc_auc_score(y_valid, y_pred)\n    return {'loss': -score, 'status': STATUS_OK}\n\nspace = {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'auc',\n    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n    'max_depth': hp.quniform('max_depth', 3, 10, 1),\n    'num_leaves': hp.quniform('num_leaves', 20, 100, 1),\n    'feature_fraction': hp.uniform('feature_fraction', 0.5, 1.0),\n    'bagging_fraction': hp.uniform('bagging_fraction', 0.5, 1.0)\n}\n\ntrials = Trials()\nbest = fmin(fn=evaluate_params,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=20,\n            trials=trials,\n            rstate=np.random.default_rng(42))\n\nprint(best)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-31T15:24:46.184521Z","iopub.execute_input":"2024-05-31T15:24:46.185185Z","iopub.status.idle":"2024-05-31T15:30:17.522867Z","shell.execute_reply.started":"2024-05-31T15:24:46.185143Z","shell.execute_reply":"2024-05-31T15:30:17.521201Z"},"trusted":true},"execution_count":140,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 28872, number of negative: 887123\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.203698 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8980                     \n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111\n[LightGBM] [Info] Start training from score -3.425111 \nTraining until validation scores don't improve for 10 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:                    \n[27]\tvalid_0's auc: 0.708813\n[LightGBM] [Info] Number of positive: 28872, number of negative: 887123          \n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.162416 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8980                                                \n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111  \n[LightGBM] [Info] Start training from score -3.425111                            \nTraining until validation scores don't improve for 10 rounds                     \nEarly stopping, best iteration is:                                               \n[71]\tvalid_0's auc: 0.708493\n[LightGBM] [Info] Number of positive: 28872, number of negative: 887123          \n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.470599 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8980                                                \n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111  \n[LightGBM] [Info] Start training from score -3.425111                            \nTraining until validation scores don't improve for 10 rounds                     \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \nEarly stopping, best iteration is:                                               \n[52]\tvalid_0's auc: 0.709317\n[LightGBM] [Info] Number of positive: 28872, number of negative: 887123          \n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.217407 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8980                                                \n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111  \n[LightGBM] [Info] Start training from score -3.425111                            \nTraining until validation scores don't improve for 10 rounds                     \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \nEarly stopping, best iteration is:                                               \n[72]\tvalid_0's auc: 0.718303\n[LightGBM] [Info] Number of positive: 28872, number of negative: 887123          \n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.450608 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8980                                                \n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111  \n[LightGBM] [Info] Start training from score -3.425111                            \nTraining until validation scores don't improve for 10 rounds                     \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[100]\tvalid_0's auc: 0.710158                                                    \nDid not meet early stopping. Best iteration is:                                  \n[100]\tvalid_0's auc: 0.710158\n[LightGBM] [Info] Number of positive: 28872, number of negative: 887123          \n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.199380 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8980                                                \n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111  \n[LightGBM] [Info] Start training from score -3.425111                            \nTraining until validation scores don't improve for 10 rounds                     \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[100]\tvalid_0's auc: 0.710552                                                    \nDid not meet early stopping. Best iteration is:                                  \n[91]\tvalid_0's auc: 0.713613\n[LightGBM] [Info] Number of positive: 28872, number of negative: 887123          \n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.205680 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8980                                                \n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111  \n[LightGBM] [Info] Start training from score -3.425111                            \nTraining until validation scores don't improve for 10 rounds                     \n[100]\tvalid_0's auc: 0.706212                                                    \nDid not meet early stopping. Best iteration is:                                  \n[99]\tvalid_0's auc: 0.706323\n[LightGBM] [Info] Number of positive: 28872, number of negative: 887123          \n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.451855 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8980                                                \n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111  \n[LightGBM] [Info] Start training from score -3.425111                            \nTraining until validation scores don't improve for 10 rounds                     \nEarly stopping, best iteration is:                                               \n[37]\tvalid_0's auc: 0.722844\n[LightGBM] [Info] Number of positive: 28872, number of negative: 887123          \n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.448056 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8980                                                \n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111  \n[LightGBM] [Info] Start training from score -3.425111                            \nTraining until validation scores don't improve for 10 rounds                     \n[100]\tvalid_0's auc: 0.699875                                                    \nDid not meet early stopping. Best iteration is:                                  \n[100]\tvalid_0's auc: 0.699875\n[LightGBM] [Info] Number of positive: 28872, number of negative: 887123          \n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.205336 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8980                                                \n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111  \n[LightGBM] [Info] Start training from score -3.425111                            \nTraining until validation scores don't improve for 10 rounds                     \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf       \nEarly stopping, best iteration is:                                               \n[67]\tvalid_0's auc: 0.71324\n[LightGBM] [Info] Number of positive: 28872, number of negative: 887123           \n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202177 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8980                                                 \n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111   \n[LightGBM] [Info] Start training from score -3.425111                             \nTraining until validation scores don't improve for 10 rounds                      \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf        \n[100]\tvalid_0's auc: 0.700804                                                     \nDid not meet early stopping. Best iteration is:                                   \n[100]\tvalid_0's auc: 0.700804\n[LightGBM] [Info] Number of positive: 28872, number of negative: 887123           \n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.435872 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8980                                                 \n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111   \n[LightGBM] [Info] Start training from score -3.425111                             \nTraining until validation scores don't improve for 10 rounds                      \nEarly stopping, best iteration is:                                                \n[2]\tvalid_0's auc: 0.654\n[LightGBM] [Info] Number of positive: 28872, number of negative: 887123           \n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.431779 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8980                                                 \n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111   \n[LightGBM] [Info] Start training from score -3.425111                             \nTraining until validation scores don't improve for 10 rounds                      \nEarly stopping, best iteration is:                                                \n[81]\tvalid_0's auc: 0.723379\n[LightGBM] [Info] Number of positive: 28872, number of negative: 887123           \n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.198450 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8980                                                 \n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111   \n[LightGBM] [Info] Start training from score -3.425111                             \nTraining until validation scores don't improve for 10 rounds                      \nEarly stopping, best iteration is:                                                \n[11]\tvalid_0's auc: 0.620362\n[LightGBM] [Info] Number of positive: 28872, number of negative: 887123           \n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.196471 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8980                                                 \n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111   \n[LightGBM] [Info] Start training from score -3.425111                             \nTraining until validation scores don't improve for 10 rounds                      \nEarly stopping, best iteration is:                                                \n[8]\tvalid_0's auc: 0.618441\n[LightGBM] [Info] Number of positive: 28872, number of negative: 887123           \n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.438113 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8980                                                 \n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111   \n[LightGBM] [Info] Start training from score -3.425111                             \nTraining until validation scores don't improve for 10 rounds                      \n[100]\tvalid_0's auc: 0.715038                                                     \nDid not meet early stopping. Best iteration is:                                   \n[99]\tvalid_0's auc: 0.715044\n[LightGBM] [Info] Number of positive: 28872, number of negative: 887123           \n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.435721 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8980                                                 \n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111   \n[LightGBM] [Info] Start training from score -3.425111                             \nTraining until validation scores don't improve for 10 rounds                      \nEarly stopping, best iteration is:                                                \n[62]\tvalid_0's auc: 0.704693\n[LightGBM] [Info] Number of positive: 28872, number of negative: 887123           \n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.193865 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 8980                                                 \n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111   \n[LightGBM] [Info] Start training from score -3.425111                             \nTraining until validation scores don't improve for 10 rounds                      \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf        \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf        \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf        \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf        \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf        \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf        \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf        \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf        \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf        \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf        \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf        \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf        \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf        \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf        \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf        \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf        \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf        \n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf        \n[100]\tvalid_0's auc: 0.717852                                                     \nDid not meet early stopping. Best iteration is:                                   \n[100]\tvalid_0's auc: 0.717852\n[LightGBM] [Info] Number of positive: 28872, number of negative: 887123           \n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.436955 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8980                                                 \n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111   \n[LightGBM] [Info] Start training from score -3.425111                             \nTraining until validation scores don't improve for 10 rounds                      \nEarly stopping, best iteration is:                                                \n[3]\tvalid_0's auc: 0.643917\n[LightGBM] [Info] Number of positive: 28872, number of negative: 887123           \n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.446739 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8980                                                 \n[LightGBM] [Info] Number of data points in the train set: 915995, number of used features: 48\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031520 -> initscore=-3.425111   \n[LightGBM] [Info] Start training from score -3.425111                             \nTraining until validation scores don't improve for 10 rounds                      \nEarly stopping, best iteration is:                                                \n[59]\tvalid_0's auc: 0.708707\n100%|██████████| 20/20 [05:31<00:00, 16.57s/trial, best loss: -0.7233793153665502]\n{'bagging_fraction': 0.9043917325109541, 'feature_fraction': 0.6084348379285371, 'learning_rate': 0.12572978832678067, 'max_depth': 8.0, 'num_leaves': 43.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"lgb_train = lgb.Dataset(X_train, label=y_train)\nlgb_valid = lgb.Dataset(X_valid, label=y_valid, reference=lgb_train)\n\nparams = {\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n    \"metric\": \"auc\",\n    \"max_depth\": 8,\n    \"num_leaves\": 43,\n    \"learning_rate\": 0.12572978832678067,\n    \"feature_fraction\": 0.6084348379285371, \n    \"bagging_fraction\": 0.9043917325109541,\n    \"bagging_freq\": 5,\n    \"n_estimators\": 1000,\n    \"verbose\": -1,\n}\n#bagging_fraction': , 'feature_fraction': , 'learning_rate': , 'max_depth': 5, 'n_estimators': 975, 'num_leaves': 48\ngbm = lgb.train(\n    params,\n    lgb_train,\n    valid_sets=lgb_valid,\n    callbacks=[lgb.log_evaluation(60), lgb.early_stopping(10)]\n)\n\nfor base, X in [(base_train, X_train), (base_valid, X_valid), (base_test, X_test)]:\n    y_pred = gbm.predict(X, num_iteration=gbm.best_iteration)\n    base[\"score\"] = y_pred\n\nprint(f'The AUC score on the train set is: {roc_auc_score(base_train[\"target\"], base_train[\"score\"])}') \nprint(f'The AUC score on the valid set is: {roc_auc_score(base_valid[\"target\"], base_valid[\"score\"])}') \nprint(f'The AUC score on the test set is: {roc_auc_score(base_test[\"target\"], base_test[\"score\"])}')  ","metadata":{"execution":{"iopub.status.busy":"2024-05-31T15:32:49.489664Z","iopub.execute_input":"2024-05-31T15:32:49.490199Z","iopub.status.idle":"2024-05-31T15:33:08.731807Z","shell.execute_reply.started":"2024-05-31T15:32:49.490153Z","shell.execute_reply":"2024-05-31T15:33:08.730123Z"},"trusted":true},"execution_count":143,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[44]\tvalid_0's auc: 0.71927\nThe AUC score on the train set is: 0.7645672890821629\nThe AUC score on the valid set is: 0.7192696884405961\nThe AUC score on the test set is: 0.7164991741742833\n","output_type":"stream"}]},{"cell_type":"code","source":"def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n    gini_in_time = base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]]\\\n        .sort_values(\"WEEK_NUM\")\\\n        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]]\\\n        .apply(lambda x: 2*roc_auc_score(x[\"target\"], x[\"score\"])-1).tolist()\n    \n    x = np.arange(len(gini_in_time))\n    y = gini_in_time\n    a, b = np.polyfit(x, y, 1)\n    y_hat = a*x + b\n    residuals = y - y_hat\n    res_std = np.std(residuals)\n    avg_gini = np.mean(gini_in_time)\n    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std\n\nstability_score_train = gini_stability(base_train)\nstability_score_valid = gini_stability(base_valid)\nstability_score_test = gini_stability(base_test)\n\nprint(f'The stability score on the train set is: {stability_score_train}') \nprint(f'The stability score on the valid set is: {stability_score_valid}') \nprint(f'The stability score on the test set is: {stability_score_test}')  ","metadata":{"execution":{"iopub.status.busy":"2024-05-31T15:33:21.962650Z","iopub.execute_input":"2024-05-31T15:33:21.963240Z","iopub.status.idle":"2024-05-31T15:33:23.307063Z","shell.execute_reply.started":"2024-05-31T15:33:21.963174Z","shell.execute_reply":"2024-05-31T15:33:23.305558Z"},"trusted":true},"execution_count":144,"outputs":[{"name":"stdout","text":"The stability score on the train set is: 0.5022621276620299\nThe stability score on the valid set is: 0.4070058676578434\nThe stability score on the test set is: 0.39337574545092435\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Avec une recherche manuelle nous avon trouvé ce modèle un peu plus performant que les autre obtenus avec des méthodes d'optimisation. Cela peut peut-etre du au choix de l'espace des hyperparamètres.","metadata":{}},{"cell_type":"code","source":"##Meilleur modèle lightgbm\nlgb_train = lgb.Dataset(X_train, label=y_train)\nlgb_valid = lgb.Dataset(X_valid, label=y_valid, reference=lgb_train)\n\nparams = {\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n    \"metric\": \"auc\",\n    \"max_depth\": 15,\n    \"num_leaves\": 48,\n    \"learning_rate\": 0.05,\n    \"feature_fraction\": 0.9, \n    \"bagging_fraction\": 0.8,\n    \"bagging_freq\": 5,\n    \"n_estimators\": 2000,\n    \"verbose\": -1,\n}\n#bagging_fraction': , 'feature_fraction': , 'learning_rate': , 'max_depth': 5, 'n_estimators': 975, 'num_leaves': 48\ngbm = lgb.train(\n    params,\n    lgb_train,\n    valid_sets=lgb_valid,\n    callbacks=[lgb.log_evaluation(60), lgb.early_stopping(10)]\n)\n\nfor base, X in [(base_train, X_train), (base_valid, X_valid), (base_test, X_test)]:\n    y_pred = gbm.predict(X, num_iteration=gbm.best_iteration)\n    base[\"score\"] = y_pred\n\nprint(f'The AUC score on the train set is: {roc_auc_score(base_train[\"target\"], base_train[\"score\"])}') \nprint(f'The AUC score on the valid set is: {roc_auc_score(base_valid[\"target\"], base_valid[\"score\"])}') \nprint(f'The AUC score on the test set is: {roc_auc_score(base_test[\"target\"], base_test[\"score\"])}')  ","metadata":{"execution":{"iopub.status.busy":"2024-05-31T15:37:16.601472Z","iopub.execute_input":"2024-05-31T15:37:16.602056Z","iopub.status.idle":"2024-05-31T15:38:07.630613Z","shell.execute_reply.started":"2024-05-31T15:37:16.602016Z","shell.execute_reply":"2024-05-31T15:38:07.629213Z"},"trusted":true},"execution_count":148,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 10 rounds\n[60]\tvalid_0's auc: 0.712763\n[120]\tvalid_0's auc: 0.722032\nEarly stopping, best iteration is:\n[142]\tvalid_0's auc: 0.723386\nThe AUC score on the train set is: 0.7824174684316207\nThe AUC score on the valid set is: 0.7233862700684043\nThe AUC score on the test set is: 0.7240282962502375\n","output_type":"stream"}]},{"cell_type":"code","source":"def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n    gini_in_time = base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]]\\\n        .sort_values(\"WEEK_NUM\")\\\n        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]]\\\n        .apply(lambda x: 2*roc_auc_score(x[\"target\"], x[\"score\"])-1).tolist()\n    \n    x = np.arange(len(gini_in_time))\n    y = gini_in_time\n    a, b = np.polyfit(x, y, 1)\n    y_hat = a*x + b\n    residuals = y - y_hat\n    res_std = np.std(residuals)\n    avg_gini = np.mean(gini_in_time)\n    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std\n\nstability_score_train = gini_stability(base_train)\nstability_score_valid = gini_stability(base_valid)\nstability_score_test = gini_stability(base_test)\n\nprint(f'The stability score on the train set is: {stability_score_train}') \nprint(f'The stability score on the valid set is: {stability_score_valid}') \nprint(f'The stability score on the test set is: {stability_score_test}')  ","metadata":{"execution":{"iopub.status.busy":"2024-05-31T15:35:26.133567Z","iopub.execute_input":"2024-05-31T15:35:26.134134Z","iopub.status.idle":"2024-05-31T15:35:27.414894Z","shell.execute_reply.started":"2024-05-31T15:35:26.134096Z","shell.execute_reply":"2024-05-31T15:35:27.413546Z"},"trusted":true},"execution_count":146,"outputs":[{"name":"stdout","text":"The stability score on the train set is: 0.5378837797386191\nThe stability score on the valid set is: 0.41289640230069946\nThe stability score on the test set is: 0.4104966787746163\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Après tous ces codes d'optimisation de modèle, c'est le dernier modèle qui est le meilleur avec un score de 41% sur les ensembles validation et test, et 53% sur le train.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, mean_absolute_error\n\n# Obtenez les prédictions de scores pour les données de test\ny_test_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n\n# Comparez les prédictions avec les vrais scores\nmse = mean_squared_error(base_test['score'], y_test_pred)\nrmse = mean_squared_error(base_test['score'], y_test_pred, squared=False)\nmae = mean_absolute_error(base_test['score'], y_test_pred)\n\n# Affichage des résultats\nprint(f\"Mean Squared Error (MSE) : {mse}\") \nprint(f\"Root Mean Squared Error (RMSE) : {rmse}\")\nprint(f\"Mean Absolute Error (MAE) : {mae}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-31T15:56:04.404273Z","iopub.execute_input":"2024-05-31T15:56:04.405593Z","iopub.status.idle":"2024-05-31T15:56:06.203720Z","shell.execute_reply.started":"2024-05-31T15:56:04.405444Z","shell.execute_reply":"2024-05-31T15:56:06.201810Z"},"trusted":true},"execution_count":152,"outputs":[{"name":"stdout","text":"Mean Squared Error (MSE) : 0.0\nRoot Mean Squared Error (RMSE) : 0.0\nMean Absolute Error (MAE) : 0.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Dans la suite, nous devons tester d'autres modèles comme le gradient boosting ou le catboost avec ces doonnées, ensuite regarder ceux qui prédisent le mieux, en enfin faire un stacking ou un mélange de modèles pour augmenter la performance.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}